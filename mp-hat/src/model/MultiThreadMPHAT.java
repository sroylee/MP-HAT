package model;

import tool.*;

import java.io.BufferedWriter;
import java.util.Random;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

import org.apache.commons.math3.distribution.GammaDistribution;

import hoang.larc.tooler.SystemTool;

import java.io.File;
import java.io.FileWriter;

public class MultiThreadMPHAT {
	// option to print out temporary results
	public static boolean opt_interest_verbose = false;
	public static boolean opt_authority_verbose = false;
	public static boolean opt_hub_verbose = false;
	public static boolean opt_platform_verbose = false;

	public static String datapath;
	public static Dataset dataset;
	public static int nTopics;
	public static int nPlatforms = 2;
	public static int batch;
	public static String outputPath;

	private static boolean userGlobalMin = true;
	private static boolean initByGroundTruth = false;
	private static boolean initByTopicModeling = true;
	private static boolean InitPlatformPreferenceByTopicModeling = false;
	private static boolean onlyLearnGibbs = false;
	private static boolean learnTopic = false;
	private static boolean learnUserInterest = true;
	private static boolean learnUserAuthority = true;
	private static boolean learnUserHub = true;
	private static boolean learnUserPlatformPreference = true;
	private static boolean asynchronousParallelUserPlatformPreference = true;
	private static boolean useLinkInLearningPlatformPreference = false;
	private static boolean usePostInLearningPlatformPreference = true;

	private static boolean usePrior = true;

	public static int gibbs_BurningPeriods = 50;
	public static int max_Gibbs_Iterations = 200; // 200
	public static int gibbs_Sampling_Gap = 20; // 10

	// priors
	public static double alpha;// prior for users' platform preferences
	public static double kappa;// prior for user's topic interests
	public static double theta;
	public static double sigma;// variance of users' authorities
	public static double delta;// variance of users' hubs
	public static double gamma; // variance of topic word distribution
	public static double epsilon = 0.000001;
	public static double lamda = 0.01;
	public static double omega = 35; // regularization for hub
	public static double phi = 1; // regularization for authority

	public static Random rand = new Random(1);

	// Gibbs sampling variables
	// user-topic counts
	public static int[][] n_zu = null; // n_zu[z][u]: number of times topic z is
	// observed in posts by user u
	public static int[] sum_nzu = null; // sum_nzu[u] total number of topics
										// that are
	// observed in posts by user u

	// topic-word counts
	public static int[][] n_zw = null; // n_wz[z][w]: number of times word w is
	// generated by topic z in a post
	public static int[] sum_nzw = null; // sum_nwz[z]: total number of times
										// words that
	// are generated by topic z in a post

	// topic-word distribution
	public static double[][] topicWordDist = null; // topicWordDist[k][w]: the
	// distribution of word w for topic
	// k. Sum of each words distribution
	// for each k = 1. This tau in our
	// paper

	public static double[][] optTopicWordDist = null; // optimized
														// topicWordDist[k][w]

	public static double globalTopicInterestsMin = Double.POSITIVE_INFINITY;

	// options for learning
	public static double lineSearch_alpha = 0.0001;
	public static double lineSearch_beta = 0.1;
	public static int lineSearch_MaxIterations = 10;
	public static double lineSearch_lambda;

	public static int maxIteration_topicalInterest = 10;
	public static int maxIteration_Authorities = 10;
	public static int maxIteration_Hubs = 10;
	public static int maxIteration_platformPreference = 10;

	public static int max_GibbsEM_Iterations = 200;

	public static int nParallelThreads = 20;
	public static int[] threadStartIndexes = null;
	public static int[] threadEndIndexes = null;

	public static double[] threadLikelihood;

	private double postLastLogLikelidhood;
	private double postLastLogPerplexity;
	private double postOptLogLikelidhood;
	private double postOptLogPerplexity;

	static class ChildThread implements Runnable {
		private int threadId;
		private int threadStartIndex;
		private int threadEndIndex;
		private String runOption;

		public ChildThread(int start, int end, String run) {
			this.threadStartIndex = start;
			this.threadEndIndex = end;
			runOption = run;
		}

		public ChildThread(int _threadId, int start, int end) {
			this.threadId = _threadId;
			this.threadStartIndex = start;
			this.threadEndIndex = end;
			runOption = "getLoglikelihood";
		}

		@Override
		public void run() {
			if (runOption.equals("optTopicInterests")) {
				optTopicalInterests(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("optAuthorities")) {
				optAuthorities(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("optHubs")) {
				optHubs(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("optPlatformPreferences")) {
				optPlatformPreferences(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("topicSample")) {
				topicSample(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("updateOpt")) {
				updateOptimalParams(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("initPostTopic")) {
				initUserPostTopic(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("getLoglikelihood")) {
				getLogLikelihood(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("randomInitUser")) {
				randomInitAuthorityHubPlatformPreference(threadStartIndex, threadEndIndex);
			} else if (runOption.equals("gibbsInitUser")) {
				gibbsInitAuthorityHubPlatformPreference(threadStartIndex, threadEndIndex);
			}
		}

		private void getLogLikelihood(int startIndex, int endIndex) {
			threadLikelihood[threadId] = 0;
			for (int u = startIndex; u < endIndex; u++) {
				threadLikelihood[threadId] += getLikelihood(u);
			}
		}

		private void initUserPostTopic(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++) {
				initPostTopic(u);
			}
		}

		private void optTopicalInterests(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++)
				altOptimize_topicalInterest(u);
		}

		private void optAuthorities(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++)
				altOptimize_Authorities(u);
		}

		private void optHubs(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++)
				altOptimize_Hubs(u);
		}

		private void optPlatformPreferences(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++)
				for (int k = 0; k < nTopics; k++) {
					altOptimize_PlatformPreference(u, k);
				}
		}

		private void topicSample(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++) {
				for (int n = 0; n < dataset.users[u].nPosts; n++) {
					// only consider posts in batch
					if (dataset.users[u].postBatches[n] == batch) {
						samplePostTopic_EMGibbs(u, n);
					}
				}
			}
		}

		private void updateOptimalParams(int startIndex, int endIndex) {
			for (int u = startIndex; u < endIndex; u++) {
				User currUser = dataset.users[u];
				for (int z = 0; z < nTopics; z++) {
					currUser.optTopicalInterests[z] = currUser.topicalInterests[z];
					currUser.optAuthorities[z] = currUser.authorities[z];
					currUser.optHubs[z] = currUser.hubs[z];
					for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
						currUser.optTopicalPlatformPreference[z][p] = currUser.topicalPlatformPreference[z][p];
					}

				}
			}
		}

		private void randomInitAuthorityHubPlatformPreference(int startIndex, int endIndex) {
			// NormalDistribution g;
			for (int u = startIndex; u < endIndex; u++) {
				User currUser = dataset.users[u];
				for (int k = 0; k < nTopics; k++) {
					// interest
					GammaDistribution g = new GammaDistribution(kappa, theta);
					currUser.topicalInterests[k] = g.sample();
					// preference
					for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
						if (currUser.platforms[p] == 1) {
							g = new GammaDistribution(alpha, theta);
							currUser.topicalPlatformPreference[k][p] = g.sample();
						} else {
							currUser.topicalPlatformPreference[k][p] = Double.NEGATIVE_INFINITY;
						}
					}
					// authority
					g = new GammaDistribution(sigma, currUser.topicalInterests[k] / sigma);
					currUser.authorities[k] = g.sample();
					// hub
					g = new GammaDistribution(delta, currUser.topicalInterests[k] / delta);
					currUser.hubs[k] = g.sample();

					//
					currUser.topicalRelativePlatformPreference[k] = MathTool
							.softmax(currUser.topicalPlatformPreference[k]);
				}
			}
		}

		private void gibbsInitAuthorityHubPlatformPreference(int startIndex, int endIndex) {
			// NormalDistribution g;
			for (int u = startIndex; u < endIndex; u++) {
				User currUser = dataset.users[u];
				double norm = 0;
				if (userGlobalMin) {
					norm = 1 / globalTopicInterestsMin + 0.1;
				} else {
					double min = Double.POSITIVE_INFINITY;
					for (int k = 0; k < nTopics; k++) {
						if (min > currUser.topicalInterests[k]) {
							min = currUser.topicalInterests[k];
						}
					}

					norm = 1 / min + 0.1;
				}

				for (int k = 0; k < nTopics; k++) {
					currUser.topicalInterests[k] = Math.log(currUser.topicalInterests[k] * norm);
				}

				if (InitPlatformPreferenceByTopicModeling) {
					for (int k = 0; k < nTopics; k++) {
						double min = Double.POSITIVE_INFINITY;
						for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
							if (min > currUser.topicalPlatformPreference[k][p]) {
								min = currUser.topicalPlatformPreference[k][p];
							}
						}
						norm = 1 / min + 0.1;

						for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
							if (currUser.platforms[p] == 1) {
								currUser.topicalPlatformPreference[k][p] = Math
										.log(currUser.topicalPlatformPreference[k][p] * norm);
							} else {
								currUser.topicalPlatformPreference[k][p] = Double.NEGATIVE_INFINITY;
							}
							// System.out.println(u + "," + k + "," + p + "," +
							// currUser.topicalPlatformPreference[k][p]);
						}

					}

				} else {
					GammaDistribution g;
					for (int k = 0; k < nTopics; k++) {
						for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
							if (currUser.platforms[p] == 1) {
								g = new GammaDistribution(alpha, theta);
								//currUser.topicalPlatformPreference[k][p] = g.sample();
								currUser.topicalPlatformPreference[k][p] = 0.5;
							} else {
								currUser.topicalPlatformPreference[k][p] = Double.NEGATIVE_INFINITY;
							}
						}
					}
				}

				for (int k = 0; k < nTopics; k++) {
					// GammaDistribution g;
					// authority
					// g = new GammaDistribution(sigma,
					// currUser.topicalInterests[k] / sigma);
					// currUser.authorities[k] = g.sample();

					currUser.authorities[k] = currUser.topicalInterests[k];
					// hub
					// g = new GammaDistribution(delta,
					// currUser.topicalInterests[k] / delta);
					// currUser.hubs[k] = g.sample();
					currUser.hubs[k] = currUser.topicalInterests[k];

					currUser.topicalRelativePlatformPreference[k] = MathTool
							.softmax(currUser.topicalPlatformPreference[k]);
				}
			}
		}
	}

	public void getThreadIndexes() {
		threadStartIndexes = new int[nParallelThreads];
		threadEndIndexes = new int[nParallelThreads];
		threadLikelihood = new double[nParallelThreads];
		int chunkLength = Math.floorDiv(dataset.nUsers, nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			threadStartIndexes[i] = i * chunkLength;
			threadEndIndexes[i] = threadStartIndexes[i] + chunkLength;
		}
		threadEndIndexes[nParallelThreads - 1] = dataset.nUsers;
		for (int i = 0; i < nParallelThreads; i++) {
			System.out.printf("thread[%d]: start = %d end = %d\n", i, threadStartIndexes[i], threadEndIndexes[i]);
		}
	}

	/***
	 * 
	 * @param _datasetPath
	 * @param _nTopics
	 */
	public MultiThreadMPHAT(String _datasetPath, int _nTopics, int _batch, String _outputPath) {
		datapath = _datasetPath;
		dataset = new Dataset(_datasetPath, _batch, false);
		nTopics = _nTopics;
		batch = _batch;
		outputPath = _outputPath;

		n_zu = new int[nTopics][dataset.nUsers];
		sum_nzu = new int[dataset.nUsers];
		n_zw = new int[nTopics][dataset.vocabulary.length];
		sum_nzw = new int[nTopics];
		topicWordDist = new double[nTopics][dataset.vocabulary.length];
	}

	/***
	 * get likelihood of the whole dataset
	 * 
	 * @return
	 */
	public static double getLikelihood() {
		// Compute the likelihood to make sure that it is improving L(text) +
		// L(link)
		// value can be more than 1
		// sum of eqn 1 -10
		double linkLikelihood = 0;
		double relationshipLikelihood = 0;
		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double platformPreferencePrior = 0;
		double postLikelihood = 0;
		double postWordLikelihood = 0;
		double postTopicLikelihood = 0;
		double postPlatformLikelihood = 0;
		double latentFactorPrior = 0;
		double topicPrior = 0;

		double temp;
		double log2 = Math.log(2);

		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			// relationship
			if (currUser.followings != null) {
				for (int i = 0; i < currUser.followings.length; i++) {
					int v = currUser.followings[i].followingIndex;
					User following = dataset.users[v];
					int p = currUser.followings[i].platform;
					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
								* following.authorities[z] * following.topicalRelativePlatformPreference[z][p];
					}
					HupAvp = HupAvp * lamda;

					// linkRelationshipLikelihood += Math.log(1 -
					// Math.exp(-HupAvp)) - Math.log(Math.exp(-HupAvp) + 1);

					temp = Math.exp(-HupAvp);
					relationshipLikelihood += Math.log(1 - temp) - Math.log(temp + 1);
				}
			}
			if (currUser.followers != null) {
				for (int i = 0; i < currUser.followers.length; i++) {
					int v = currUser.followers[i].followerIndex;
					User follower = dataset.users[v];
					int p = currUser.followers[i].platform;
					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p]
								* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];

					}
					HupAvp = HupAvp * lamda;

					// linkRelationshipLikelihood += Math.log(1 -
					// Math.exp(-HupAvp)) - Math.log(Math.exp(-HupAvp) + 1);

					temp = Math.exp(-HupAvp);
					relationshipLikelihood += Math.log(1 - temp) - Math.log(temp + 1);
				}

			}
			if (currUser.nonFollowings != null) {
				for (int i = 0; i < currUser.nonFollowings.length; i++) {
					int v = currUser.nonFollowings[i].followingIndex;
					User nonFollowing = dataset.users[v];
					int p = currUser.nonFollowings[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
								* nonFollowing.authorities[z] * nonFollowing.topicalRelativePlatformPreference[z][p];

					}
					HupAvp = HupAvp * lamda;
					// linkRelationshipLikelihood += Math.log(2) - HupAvp -
					// Math.log(Math.exp(-HupAvp) + 1);
					relationshipLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);

				}
			}
			if (currUser.nonFollowers != null) {
				for (int i = 0; i < currUser.nonFollowers.length; i++) {

					int v = currUser.nonFollowers[i].followerIndex;
					User nonFollower = dataset.users[v];
					int p = currUser.nonFollowers[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p]
								* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];

					}
					HupAvp = HupAvp * lamda;
					// linkRelationshipLikelihood += Math.log(2) - HupAvp -
					// Math.log(Math.exp(-HupAvp) + 1);

					relationshipLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);
				}
			}

			for (int k = 0; k < nTopics; k++) {
				// authority likelihood given x
				authorityLikelihood += ((sigma - 1) * Math.log(currUser.authorities[k]))
						- ((currUser.authorities[k] * sigma) / currUser.topicalInterests[k])
						- (sigma * Math.log(currUser.topicalInterests[k]));
				if (Double.isInfinite(authorityLikelihood) || Double.isNaN(authorityLikelihood)) {
					System.out.printf("[authority] A[%d] = %.12f\n", k, currUser.authorities[k]);
					System.exit(0);
				}
				// hub likelihood given x
				hubLikelihood += ((delta - 1) * Math.log(currUser.hubs[k]))
						- ((currUser.hubs[k] * delta) / currUser.topicalInterests[k])
						- (delta * Math.log(currUser.topicalInterests[k]));
				if (Double.isInfinite(hubLikelihood) || Double.isNaN(hubLikelihood)) {
					System.out.printf("[hub] H[%d] = %.12f\n", k, currUser.hubs[k]);
					System.exit(0);
				}
			}

			// posts
			double topicDenominator = 0;
			double[] platformDenominators = new double[nTopics];
			for (int k = 0; k < nTopics; k++) {
				topicDenominator += Math.exp(currUser.topicalInterests[k]);
				platformDenominators[k] = 0;
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					if (currUser.platforms[p] == 0) {
						continue;
					}
					platformDenominators[k] += Math.exp(currUser.topicalPlatformPreference[k][p]);
				}
				platformDenominators[k] = Math.log(platformDenominators[k]);
			}
			topicDenominator = Math.log(topicDenominator);

			for (int s = 0; s < currUser.nPosts; s++) {
				if (currUser.postBatches[s] == batch) {
					Post currPost = currUser.posts[s];
					// words
					for (int w = 0; w < currPost.words.length; w++) {
						int word = currPost.words[w];
						postWordLikelihood += Math.log(topicWordDist[currPost.topic][word]);
					}

					// platform

					// double denominator = 0;
					// for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					// denominator +=
					// Math.exp(currUser.topicalPlatformPreference[currPost.topic][p]);
					// }

					// postPlatformLikelihood += Math
					// .log(Math.exp(currUser.topicalPlatformPreference[currPost.topic][currPost.platform])
					// / denominator);
					postPlatformLikelihood += currUser.topicalPlatformPreference[currPost.topic][currPost.platform]
							- platformDenominators[currPost.topic];

					if (Double.isInfinite(postPlatformLikelihood) || Double.isNaN(postPlatformLikelihood)) {
						System.out.printf("[Post] platform[%d] = %.12f\n", currPost.platform,
								currUser.topicalPlatformPreference[currPost.topic][currPost.platform]);
						System.exit(0);
					}

					// topic

					// denominator = 0;
					// for (int k = 0; k < nTopics; k++) {
					// denominator +=
					// Math.exp(currUser.topicalInterests[currPost.topic]);
					// }
					// postTopicLikelihood +=
					// Math.log(Math.exp(currUser.topicalInterests[currPost.topic])
					// / denominator);

					postTopicLikelihood += currUser.topicalInterests[currPost.topic] - topicDenominator;
					if (Double.isInfinite(postTopicLikelihood) || Double.isNaN(postTopicLikelihood)) {
						System.out.printf("[Post] topic[%d] = %.12f\n", currPost.topic,
								currUser.topicalInterests[currPost.topic]);
						System.exit(0);
					}
				}
			}

			if (usePrior) {
				for (int k = 0; k < nTopics; k++) {
					// users' topical interest prior
					latentFactorPrior += ((kappa - 1) * Math.log(currUser.topicalInterests[k]))
							- (currUser.topicalInterests[k] / theta);
					// platform preference prior
					for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
						if (currUser.platforms[p] == 0) {
							continue;
						}
						platformPreferencePrior += (alpha - 1) * Math.log(currUser.topicalPlatformPreference[k][p])
								- (currUser.topicalPlatformPreference[k][p] / theta);
						if (Double.isInfinite(platformPreferencePrior) || Double.isNaN(platformPreferencePrior)) {
							System.out.printf("[platform] P[%d] = %.12f\n", k,
									currUser.topicalPlatformPreference[k][p]);
							System.exit(0);
						}
					}
				}
			}
		}

		if (usePrior) {// topics' prior
			for (int k = 0; k < nTopics; k++) {
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					topicPrior += (gamma - 1) * Math.log(topicWordDist[k][w]);
				}
			}
		}

		linkLikelihood += relationshipLikelihood + authorityLikelihood + hubLikelihood + platformPreferencePrior;

		postLikelihood = postWordLikelihood + postPlatformLikelihood + postTopicLikelihood + latentFactorPrior
				+ topicPrior;

		if (Double.isInfinite(linkLikelihood) || Double.isInfinite(postLikelihood) || Double.isNaN(linkLikelihood)
				|| Double.isNaN(postLikelihood)) {
			System.out.println("either linkLikelihood or postLikelihood is Infinite or NAN");
			System.exit(-1);
		}

		return (linkLikelihood + postLikelihood);
	}

	public static double getLikelihood_parallel() {
		double loglikelihood = 0;
		ExecutorService executor = Executors.newFixedThreadPool(nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			Runnable worker = new ChildThread(i, threadStartIndexes[i], threadEndIndexes[i]);
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
			// do nothing, just wait for the threads to finish
		}
		for (int i = 0; i < nParallelThreads; i++) {
			loglikelihood += threadLikelihood[i];
		}
		if (usePrior) {// topics' prior
			for (int k = 0; k < nTopics; k++) {
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					loglikelihood += (gamma - 1) * Math.log(topicWordDist[k][w]);
				}
			}
		}
		return loglikelihood;
	}

	public static double getLikelihood(int u) {
		double linkLikelihood = 0;
		double linkRelationshipLikelihood = 0;
		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double platformPreferencePrior = 0;
		double postLikelihood = 0;
		double postWordLikelihood = 0;
		double postTopicLikelihood = 0;
		double postPlatformLikelihood = 0;
		double latentFactorPrior = 0;

		double temp;
		double log2 = Math.log(2);

		User currUser = dataset.users[u];

		// relationship
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				int v = currUser.followings[i].followingIndex;
				User following = dataset.users[v];
				int p = currUser.followings[i].platform;
				// Compute H_u^p * A_v^p
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
							* following.authorities[z] * following.topicalRelativePlatformPreference[z][p];
				}
				HupAvp = HupAvp * lamda;

				// linkRelationshipLikelihood += Math.log(1 -
				// Math.exp(-HupAvp)) - Math.log(Math.exp(-HupAvp) + 1);

				temp = Math.exp(-HupAvp);
				linkRelationshipLikelihood += Math.log(1 - temp) - Math.log(temp + 1);
			}
		}
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				int v = currUser.followers[i].followerIndex;
				User follower = dataset.users[v];
				int p = currUser.followers[i].platform;
				// Compute H_u^p * A_v^p
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p]
							* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];

				}
				HupAvp = HupAvp * lamda;

				// linkRelationshipLikelihood += Math.log(1 -
				// Math.exp(-HupAvp)) - Math.log(Math.exp(-HupAvp) + 1);

				temp = Math.exp(-HupAvp);
				linkRelationshipLikelihood += Math.log(1 - temp) - Math.log(temp + 1);
			}

		}
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				int v = currUser.nonFollowings[i].followingIndex;
				User nonFollowing = dataset.users[v];
				int p = currUser.nonFollowings[i].platform;

				// Compute H_u^p * A_v^p
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
							* nonFollowing.authorities[z] * nonFollowing.topicalRelativePlatformPreference[z][p];

				}
				HupAvp = HupAvp * lamda;
				// linkRelationshipLikelihood += Math.log(2) - HupAvp -
				// Math.log(Math.exp(-HupAvp) + 1);
				linkRelationshipLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);

			}
		}
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {

				int v = currUser.nonFollowers[i].followerIndex;
				User nonFollower = dataset.users[v];
				int p = currUser.nonFollowers[i].platform;

				// Compute H_u^p * A_v^p
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p]
							* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];

				}
				HupAvp = HupAvp * lamda;
				// linkRelationshipLikelihood += Math.log(2) - HupAvp -
				// Math.log(Math.exp(-HupAvp) + 1);

				linkRelationshipLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);
			}
		}

		for (int k = 0; k < nTopics; k++) {
			// authority prior
			authorityLikelihood += ((sigma - 1) * Math.log(currUser.authorities[k]))
					- ((currUser.authorities[k] * sigma) / currUser.topicalInterests[k])
					- (sigma * Math.log(currUser.topicalInterests[k]));
			if (Double.isInfinite(authorityLikelihood) || Double.isNaN(authorityLikelihood)) {
				System.out.printf("[authority] A[%d] = %.12f\n", k, currUser.authorities[k]);
				System.exit(0);
			}
			// hub prior
			hubLikelihood += ((delta - 1) * Math.log(currUser.hubs[k]))
					- ((currUser.hubs[k] * delta) / currUser.topicalInterests[k])
					- (delta * Math.log(currUser.topicalInterests[k]));
			if (Double.isInfinite(hubLikelihood) || Double.isNaN(hubLikelihood)) {
				System.out.printf("[hub] H[%d] = %.12f\n", k, currUser.hubs[k]);
				System.exit(0);
			}
		}

		// posts
		double topicDenominator = 0;
		double[] platformDenominators = new double[nTopics];
		for (int k = 0; k < nTopics; k++) {
			topicDenominator += Math.exp(currUser.topicalInterests[k]);
			platformDenominators[k] = 0;
			for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
				if (currUser.platforms[p] == 0) {
					continue;
				}
				platformDenominators[k] += Math.exp(currUser.topicalPlatformPreference[k][p]);
			}
			platformDenominators[k] = Math.log(platformDenominators[k]);
		}
		topicDenominator = Math.log(topicDenominator);

		for (int s = 0; s < currUser.nPosts; s++) {
			if (currUser.postBatches[s] == batch) {
				Post currPost = currUser.posts[s];
				// words
				for (int w = 0; w < currPost.words.length; w++) {
					int word = currPost.words[w];
					postWordLikelihood += Math.log(topicWordDist[currPost.topic][word]);
				}

				// platform

				// double denominator = 0;
				// for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
				// denominator +=
				// Math.exp(currUser.topicalPlatformPreference[currPost.topic][p]);
				// }

				// postPlatformLikelihood += Math
				// .log(Math.exp(currUser.topicalPlatformPreference[currPost.topic][currPost.platform])
				// / denominator);
				postPlatformLikelihood += currUser.topicalPlatformPreference[currPost.topic][currPost.platform]
						- platformDenominators[currPost.topic];

				if (Double.isInfinite(postPlatformLikelihood) || Double.isNaN(postPlatformLikelihood)) {
					System.out.printf("[Post] platform[%d] = %.12f\n", currPost.platform,
							currUser.topicalPlatformPreference[currPost.topic][currPost.platform]);
					System.exit(0);
				}

				// topic

				// denominator = 0;
				// for (int k = 0; k < nTopics; k++) {
				// denominator +=
				// Math.exp(currUser.topicalInterests[currPost.topic]);
				// }
				// postTopicLikelihood +=
				// Math.log(Math.exp(currUser.topicalInterests[currPost.topic])
				// / denominator);

				postTopicLikelihood += currUser.topicalInterests[currPost.topic] - topicDenominator;
				if (Double.isInfinite(postTopicLikelihood) || Double.isNaN(postTopicLikelihood)) {
					System.out.printf("[Post] topic[%d] = %.12f\n", currPost.topic,
							currUser.topicalInterests[currPost.topic]);
					System.exit(0);
				}
			}
		}

		if (usePrior) {
			for (int k = 0; k < nTopics; k++) {
				// latent factor prior
				latentFactorPrior += ((kappa - 1) * Math.log(currUser.topicalInterests[k]))
						- (currUser.topicalInterests[k] / theta);
				// platform prior
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					if (currUser.platforms[p] == 0) {
						continue;
					}
					platformPreferencePrior += (alpha - 1) * Math.log(currUser.topicalPlatformPreference[k][p])
							- (currUser.topicalPlatformPreference[k][p] / theta);
					if (Double.isInfinite(platformPreferencePrior) || Double.isNaN(platformPreferencePrior)) {
						System.out.printf("[platform] P[%d] = %.12f\n", k, currUser.topicalPlatformPreference[k][p]);
						System.exit(0);
					}
				}
			}
		}

		linkLikelihood += linkRelationshipLikelihood + authorityLikelihood + hubLikelihood + platformPreferencePrior;

		postLikelihood = postWordLikelihood + postPlatformLikelihood + postTopicLikelihood + latentFactorPrior;

		if (Double.isInfinite(linkLikelihood) || Double.isInfinite(postLikelihood) || Double.isNaN(linkLikelihood)
				|| Double.isNaN(postLikelihood)) {
			System.out.println("either linkLikelihood or postLikelihood is Infinite or NAN");
			System.exit(-1);
		}

		return (linkLikelihood + postLikelihood);
	}

	/***
	 * compute likelihood of data as a function of topical interest of u when
	 * the interest is x, i.e., if L(data|parameters) = f(X_u) + const-of-X_u
	 * then this function returns f(x)
	 * 
	 * @param u
	 * @return
	 */
	private static double getLikelihood_topicalInterest(int u, double[] x) {
		// Refer to Eqn 16 in Learning paper for Formula

		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double postLikelihood = 0;
		double prior = 0;
		double finalLikelihood = 0;
		double denominator = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];
		double temp;
		for (int k = 0; k < nTopics; k++) {
			temp = Math.log(x[k]);
			// First term in eqn 16
			// hubLikelihood += -((currUser.hubs[k] * delta) / x[k]) - (delta *
			// Math.log(x[k]));
			hubLikelihood += -((currUser.hubs[k] * delta) / x[k]) - (delta * temp);

			// Second term in eqn 16
			// authorityLikelihood += -((currUser.authorities[k] * sigma) /
			// x[k]) - (sigma * Math.log(x[k]));
			authorityLikelihood += -((currUser.authorities[k] * sigma) / x[k]) - (sigma * temp);

			if (usePrior) {
				// Fourth term in eqn 16
				// topicLikelihood += ((kappa - 1) * Math.log(x[k])) - (x[k] /
				// theta);
				prior += ((kappa - 1) * temp) - (x[k] / theta);
			}

			// denominator of third term in eqn 16
			denominator += Math.exp(x[k]);

		}

		double logDenominator = Math.log(denominator);
		for (int i = 0; i < currUser.nPosts; i++) {
			// Only compute post likelihood of posts which are in batch (i.e.
			// training batch = 1)
			if (currUser.postBatches[i] == batch) {

				// Third term in eqn 16
				int postTopic = currUser.posts[i].topic;
				// postLikelihood += Math.log(Math.exp(x[postTopic]) /
				// denominator);
				postLikelihood += x[postTopic] - logDenominator;
			}
		}
		finalLikelihood = authorityLikelihood + hubLikelihood + postLikelihood + prior;

		return finalLikelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to interest of u in
	 * topic k when the interest is x, i.e., if if L(data|parameters) = f(X_u) +
	 * const-of-X_u then this function return df/dX_uk at X_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private static double gradLikelihood_topicalInterest(int u, int k, double x) {
		// Refer to Eqn 18 in Learning paper
		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double postLikelihood = 0;
		double prior = 0;
		double gradLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		// First term in eqn 18
		hubLikelihood = ((currUser.hubs[k] * delta) / Math.pow(x, 2)) - (delta / x);

		// Second term in eqn 18
		authorityLikelihood = ((currUser.authorities[k] * sigma) / Math.pow(x, 2)) - (sigma / x);

		// Third term in eqn 18
		double first_sub_term = 0;
		double second_sub_term = 0;
		double denominator = 0;
		double temp = Math.exp(x);
		for (int z = 0; z < nTopics; z++) {
			if (z == k) {
				// denominator += Math.exp(x);
				denominator += temp;
			} else {
				denominator += Math.exp(currUser.topicalInterests[z]);
			}
		}
		for (int i = 0; i < currUser.nPosts; i++) {
			// Only compute post likelihood of posts which are in batch (i.e.
			// training batch = 1)
			if (currUser.postBatches[i] == batch) {
				// Only consider posts which are assigned topic k (i.e. z_{v,s}
				// = k)

				if (currUser.posts[i].topic == k) {
					first_sub_term++;
				}
				// second_sub_term += (1 / denominator) * Math.exp(x);
				second_sub_term += (1 / denominator) * temp;
			}

		}
		postLikelihood = first_sub_term - second_sub_term;

		// Fourth term in eqn 18
		if (usePrior) {
			prior = ((kappa - 1) / x) - (1 / theta);
		}

		gradLikelihood = authorityLikelihood + hubLikelihood + postLikelihood + prior;

		return gradLikelihood;

	}

	/***
	 * alternating step to optimize topical interest of u
	 * 
	 * @param u
	 */
	private static void altOptimize_topicalInterest(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].topicalInterests;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_topicalInterest(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		for (int iter = 0; iter < maxIteration_topicalInterest; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_topicalInterest(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
				}

				// x = tool.MathTool.softmax(x);
				// x = simplexProjection(x, 1);

				// this step to make sure that we compute f at the new x
				f = 0 - getLikelihood_topicalInterest(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				if (opt_interest_verbose) {
					System.out.printf("alt_topic: u = %d iter = %d f = %f\n", u, iter, f);
				}
			} else {
				// to see if F actually reduce after every iteration
				if (opt_interest_verbose) {
					System.out.printf("alt_topic: u = %d iter = %d f = %f\n", u, iter, f);
				}
				break;// cannot improve further
			}
		}
	}

	/***
	 * compute likelihood of data as a function of authority of u when the
	 * authority is x, i.e., if L(data|parameters) = f(A_u) + const-of-A_u then
	 * this function returns f(x)
	 * 
	 * @param v
	 * @param x[]
	 * @return
	 */
	private static double getLikelihood_authority(int v, double[] x) {
		// Refer to Eqn 24 in Learning paper
		double followerLikelihood = 0;
		double nonFollowerLikelihood = 0;
		double authorityLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[v];
		double log2 = Math.log(2);
		double temp;
		// First term in eqn 24. Compute follower likelihood.
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				int u = currUser.followers[i].followerIndex;
				User follower = dataset.users[u];
				int p = currUser.followers[i].platform;

				// Compute H_u^p * A_v^p
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p] * x[z]
							* currUser.topicalRelativePlatformPreference[z][p];// now
					// A_v
					// is
					// x4
				}
				HupAvp = HupAvp * lamda;
				// double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
				// followerLikelihood += Math.log(fHupAvp);

				// followerLikelihood += Math.log(1 - Math.exp(-HupAvp)) -
				// Math.log(Math.exp(-HupAvp) + 1);

				temp = Math.exp(-HupAvp);
				followerLikelihood += Math.log(1 - temp) - Math.log(temp + 1);
			}
		}
		// Second term in eqn 24. Compute non follower likelihood.
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				int u = currUser.nonFollowers[i].followerIndex;
				User nonFollower = dataset.users[u];
				int p = currUser.nonFollowers[i].platform;

				// Compute H_u * A_v
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p] * x[z]
							* currUser.topicalRelativePlatformPreference[z][p];// now
					// A_v
					// is
					// x
				}
				HupAvp = HupAvp * lamda;
				// double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
				// nonFollowerLikelihood += Math.log(1 - fHupAvp);
				// nonFollowerLikelihood += Math.log(2) - HupAvp -
				// Math.log(Math.exp(-HupAvp) + 1);

				nonFollowerLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);

			}
		}

		// Third term in eqn 24. Compute post likelihood.
		for (int k = 0; k < nTopics; k++) {
			authorityLikelihood += ((sigma - 1) * Math.log(x[k])) - ((x[k] * sigma) / currUser.topicalInterests[k]);// now
			// A_v
			// is
			// x
		}

		likelihood = nonFollowerLikelihood + followerLikelihood + (phi * authorityLikelihood);

		return likelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to authority of u in
	 * topic k when the authority is x, i.e., if if L(data|parameters) = f(A_u)
	 * + const-of-A_u then this function return df/dA_uk at A_uk = x
	 * 
	 * @param v
	 * @param k
	 * @param x
	 * @return
	 */
	private static double gradLikelihood_authority(int v, int k, double x) {
		// Refer to Eqn 26 in Learning paper
		double followerLikelihood = 0;
		double nonFollowerLikelihood = 0;
		double authorityLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[v];
		double temp;
		// First term in eqn 26. Compute follower likelihood
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				int u = currUser.followers[i].followerIndex;
				User follower = dataset.users[u];
				int p = currUser.followers[i].platform;

				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					if (z == k) {
						HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p] * x
								* currUser.topicalRelativePlatformPreference[z][p];
					} else {
						HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p]
								* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];
					}
				}
				HupAvp = HupAvp * lamda;

				// followerLikelihood += (1 / (1 - Math.exp(-HupAvp)) *
				// -Math.exp(-HupAvp) * -lamda
				// * follower.topicalPlatformPreference[k][p] * follower.hubs[k]
				// * currUser.topicalPlatformPreference[k][p])
				// - (1 / (Math.exp(-HupAvp) + 1) * Math.exp(-HupAvp) * -lamda
				// * follower.topicalPlatformPreference[k][p] * follower.hubs[k]
				// * currUser.topicalPlatformPreference[k][p]);
				temp = Math.exp(-HupAvp);
				followerLikelihood += (1 / (1 - temp) * -temp * -lamda
						* follower.topicalRelativePlatformPreference[k][p] * follower.hubs[k]
						* currUser.topicalRelativePlatformPreference[k][p])
						- (1 / (temp + 1) * temp * -lamda * follower.topicalRelativePlatformPreference[k][p]
								* follower.hubs[k] * currUser.topicalRelativePlatformPreference[k][p]);
			}
		}

		// Second term in eqn 26. Compute non follower likelihood
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				int u = currUser.nonFollowers[i].followerIndex;
				User nonFollower = dataset.users[u];
				int p = currUser.nonFollowers[i].platform;

				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					if (z == k) {
						HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p] * x
								* currUser.topicalRelativePlatformPreference[z][p];
					} else {
						HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p]
								* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];
					}
				}
				HupAvp = HupAvp * lamda;

				// nonFollowerLikelihood += (-lamda *
				// currUser.topicalPlatformPreference[k][p] *
				// nonFollower.hubs[k]
				// * nonFollower.topicalPlatformPreference[k][p])
				// - ((1 / (Math.exp(-HupAvp) + 1)) * (Math.exp(-HupAvp))
				// * (-lamda * nonFollower.hubs[k] *
				// nonFollower.topicalPlatformPreference[k][p]
				// * currUser.topicalPlatformPreference[k][p]));

				temp = Math.exp(-HupAvp);
				nonFollowerLikelihood += (-lamda * currUser.topicalRelativePlatformPreference[k][p]
						* nonFollower.hubs[k] * nonFollower.topicalRelativePlatformPreference[k][p])
						- ((1 / (temp + 1)) * temp
								* (-lamda * nonFollower.hubs[k] * nonFollower.topicalRelativePlatformPreference[k][p]
										* currUser.topicalRelativePlatformPreference[k][p]));
			}
		}

		// Third term in eqn 26. Compute post likelihood
		authorityLikelihood = ((sigma - 1) / x) - (sigma / currUser.topicalInterests[k]);

		gradLikelihood = nonFollowerLikelihood + followerLikelihood + (phi * authorityLikelihood);

		return gradLikelihood;
	}

	/***
	 * alternating step to optimize authorities of user u
	 * 
	 * @param u
	 */
	private static void altOptimize_Authorities(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].authorities;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_authority(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		for (int iter = 0; iter < maxIteration_Authorities; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_authority(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
					if (x[k] < epsilon) {
						x[k] = epsilon;
					}
				}

				// compute f at the new x
				f = 0 - getLikelihood_authority(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				if (opt_authority_verbose) {
					System.out.printf("alt_authority: u = %d iter = %d f = %f\n", u, iter, f);
				}
			} else {
				// to see if F actually reduce after every iteration
				if (opt_authority_verbose) {
					System.out.printf("alt_authority: u = %d iter = %d f = %f\n", u, iter, f);
				}
				break;// cannot improve further
			}
		}

	}

	/***
	 * compute likelihood of data as a function of hub of u when the hub is x,
	 * i.e., if L(data|parameters) = f(H_u) + const-of-H_u then this function
	 * returns f(x)
	 * 
	 * @param u
	 * @param x[]
	 * @return
	 */
	private static double getLikelihood_hub(int u, double[] x) {
		// Refer to Eqn 20 in learning paper
		double followingLikelihood = 0;
		double nonFollowingLikelihood = 0;
		double hubLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[u];
		double temp;
		double log2 = Math.log(2);

		// First term in eqn 20. Compute following likelihood.
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				int v = currUser.followings[i].followingIndex;
				User following = dataset.users[v];
				int p = currUser.followings[i].platform;

				// Compute H_u^p * A_v^p
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += x[z] * currUser.topicalRelativePlatformPreference[z][p] * following.authorities[z]
							* following.topicalRelativePlatformPreference[z][p];// now
					// H_u
					// is
					// x
				}
				HupAvp = HupAvp * lamda;
				// double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
				// followingLikelihood += Math.log(fHupAvp);

				// followingLikelihood += Math.log(1 - Math.exp(-HupAvp)) -
				// Math.log(Math.exp(-HupAvp) + 1);

				temp = Math.exp(-HupAvp);
				followingLikelihood += Math.log(1 - temp) - Math.log(temp + 1);

			}
		}
		// Second term in eqn 20. Compute non following likelihood.
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				int v = currUser.nonFollowings[i].followingIndex;
				User nonFollowing = dataset.users[v];
				int p = currUser.nonFollowings[i].platform;
				// Compute H_u * A_v
				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					HupAvp += x[z] * currUser.topicalRelativePlatformPreference[z][p] * nonFollowing.authorities[z]
							* nonFollowing.topicalRelativePlatformPreference[z][p];// now
					// H_u
					// is
					// x
				}
				HupAvp = HupAvp * lamda;
				// double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
				// nonFollowingLikelihood += Math.log(1 - fHupAvp);

				// nonFollowingLikelihood += Math.log(2) - HupAvp -
				// Math.log(Math.exp(-HupAvp) + 1);

				nonFollowingLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);

			}
		}

		// Third term in eqn 20. Compute post likelihood.
		for (int k = 0; k < nTopics; k++) {
			hubLikelihood += ((delta - 1) * Math.log(x[k])) - ((x[k] * delta) / currUser.topicalInterests[k]);// now
			// H_u
			// is
			// x
		}

		likelihood = nonFollowingLikelihood + followingLikelihood + (omega * hubLikelihood);
		// likelihood = followingLikelihood;

		return likelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to hub of u in topic
	 * k when the hub is x, i.e., if if L(data|parameters) = f(H_u) +
	 * const-of-H_u then this function return df/dH_uk at H_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private static double gradLikelihood_hub(int u, int k, double x) {
		// Refer to Eqn 22 in learning paper
		double followingLikelihood = 0;
		double nonFollowingLikelihood = 0;
		double hubLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];
		double temp;
		// First term in eqn 22. Compute following likelihood
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				int v = currUser.followings[i].followingIndex;
				User following = dataset.users[v];
				int p = currUser.followings[i].platform;

				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					if (z == k) {
						HupAvp += x * currUser.topicalRelativePlatformPreference[z][p] * following.authorities[z]
								* following.topicalRelativePlatformPreference[z][p];
					} else {
						HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
								* following.authorities[z] * following.topicalRelativePlatformPreference[z][p];
					}
				}
				HupAvp = HupAvp * lamda;

				// followingLikelihood += (1 / (1 - Math.exp(-HupAvp)) *
				// -Math.exp(-HupAvp)
				// * (-lamda * following.authorities[k] *
				// following.topicalPlatformPreference[k][p]
				// * currUser.topicalPlatformPreference[k][p]))
				// - (1 / (Math.exp(-HupAvp) + 1) * Math.exp(-HupAvp)
				// * (-lamda * following.authorities[k] *
				// following.topicalPlatformPreference[k][p]
				// * currUser.topicalPlatformPreference[k][p]));

				temp = Math.exp(-HupAvp);
				followingLikelihood += (1 / (1 - temp) * -temp
						* (-lamda * following.authorities[k] * following.topicalRelativePlatformPreference[k][p]
								* currUser.topicalRelativePlatformPreference[k][p]))
						- (1 / (temp + 1) * temp
								* (-lamda * following.authorities[k] * following.topicalRelativePlatformPreference[k][p]
										* currUser.topicalRelativePlatformPreference[k][p]));
			}
		}

		// Second term in eqn 22. Compute non following likelihood
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				int v = currUser.nonFollowings[i].followingIndex;
				User nonFollowing = dataset.users[v];
				int p = currUser.nonFollowings[i].platform;

				double HupAvp = 0;
				for (int z = 0; z < nTopics; z++) {
					if (z == k) {
						HupAvp += x * currUser.topicalRelativePlatformPreference[z][p] * nonFollowing.authorities[z]
								* nonFollowing.topicalRelativePlatformPreference[z][p];
					} else {
						HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
								* nonFollowing.authorities[z] * nonFollowing.topicalRelativePlatformPreference[z][p];
					}
				}
				HupAvp = HupAvp * lamda;

				// nonFollowingLikelihood += (-lamda *
				// currUser.topicalPlatformPreference[k][p]
				// * nonFollowing.authorities[k] *
				// nonFollowing.topicalPlatformPreference[k][p])
				// - ((1 / (Math.exp(-HupAvp) + 1)) * (Math.exp(-HupAvp))
				// * (-lamda * nonFollowing.authorities[k] *
				// nonFollowing.topicalPlatformPreference[k][p]
				// * currUser.topicalPlatformPreference[k][p]));

				temp = Math.exp(-HupAvp);
				nonFollowingLikelihood += (-lamda * currUser.topicalRelativePlatformPreference[k][p]
						* nonFollowing.authorities[k] * nonFollowing.topicalRelativePlatformPreference[k][p])
						- ((1 / (temp + 1)) * temp
								* (-lamda * nonFollowing.authorities[k]
										* nonFollowing.topicalRelativePlatformPreference[k][p]
										* currUser.topicalRelativePlatformPreference[k][p]));

			}
		}

		// Third term in eqn 22. Compute post likelihood
		hubLikelihood = ((delta - 1) / x) - (delta / currUser.topicalInterests[k]);

		gradLikelihood = nonFollowingLikelihood + followingLikelihood + (omega * hubLikelihood);
		// gradLikelihood = followingLikelihood;

		return gradLikelihood;
	}

	/***
	 * alternating step to optimize hubs of user u
	 * 
	 * @param u
	 */
	private static void altOptimize_Hubs(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].hubs;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_hub(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		for (int iter = 0; iter < maxIteration_Hubs; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_hub(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
					if (x[k] < epsilon) {
						x[k] = epsilon;
					}
				}

				// compute f at the new x
				f = 0 - getLikelihood_hub(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				if (opt_hub_verbose) {
					System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u, iter, f);
				}
			} else {
				// to see if F actually reduce after every iteration
				if (opt_hub_verbose) {
					System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u, iter, f);
				}
				break;// cannot improve further
			}
		}
	}

	/***
	 * compute likelihood of data as a function of platform preference for topic
	 * k of u when the preference is x, i.e., if L(data|parameters) = f(Eta_uk)
	 * + const-of-Eta_uk then this function returns f(x)
	 * 
	 * @param u
	 * @return
	 */
	private static double getLikelihood_platformPreference(int u, int k, double[] x) {
		// Refer to Eqn 28 in Learning paper for Formula
		double linkLikelihood = 0;
		double nonLinkLikelihood = 0;
		double postLikelihood = 0;
		double platformPreferencePrior = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[u];
		double[] topicalRelativePlatformPreference = MathTool.softmax(x);
		// *********link part******
		if (useLinkInLearningPlatformPreference) {
			double temp;
			double log2 = Math.log(2);
			// First term in eqn 28. Compute link likelihood.
			if (currUser.followings != null) {
				for (int i = 0; i < currUser.followings.length; i++) {
					int v = currUser.followings[i].followingIndex;
					User following = dataset.users[v];
					int p = currUser.followings[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += currUser.hubs[z] * topicalRelativePlatformPreference[p] * following.authorities[z]
									* following.topicalRelativePlatformPreference[z][p];
						} else {
							HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
									* following.authorities[z] * following.topicalRelativePlatformPreference[z][p];
						}

					}
					HupAvp = HupAvp * lamda;
					temp = Math.exp(-HupAvp);

					linkLikelihood += Math.log(1 - temp) - Math.log(temp + 1);
				}
			}
			if (currUser.followers != null) {
				for (int i = 0; i < currUser.followers.length; i++) {
					int v = currUser.followers[i].followerIndex;
					User follower = dataset.users[v];
					int p = currUser.followers[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * topicalRelativePlatformPreference[p];
						} else {
							HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];
						}
					}
					HupAvp = HupAvp * lamda;
					temp = Math.exp(-HupAvp);

					linkLikelihood += Math.log(1 - temp) - Math.log(temp + 1);
				}
			}

			// Second term in eqn 28. Compute non link likelihood.
			if (currUser.nonFollowings != null) {
				for (int i = 0; i < currUser.nonFollowings.length; i++) {
					int v = currUser.nonFollowings[i].followingIndex;
					User nonFollowing = dataset.users[v];
					int p = currUser.nonFollowings[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += currUser.hubs[z] * topicalRelativePlatformPreference[p]
									* nonFollowing.authorities[z]
									* nonFollowing.topicalRelativePlatformPreference[z][p];
						} else {
							HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
									* nonFollowing.authorities[z]
									* nonFollowing.topicalRelativePlatformPreference[z][p];
						}
					}
					HupAvp = HupAvp * lamda;

					nonLinkLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);
				}
			}
			if (currUser.nonFollowers != null) {
				for (int i = 0; i < currUser.nonFollowers.length; i++) {
					int v = currUser.nonFollowers[i].followerIndex;
					User nonFollower = dataset.users[v];
					int p = currUser.nonFollowers[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * topicalRelativePlatformPreference[p];
						} else {
							HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];
						}
					}
					HupAvp = HupAvp * lamda;

					nonLinkLikelihood += log2 - HupAvp - Math.log(Math.exp(-HupAvp) + 1);
				}
			}
		}
		// ******post part**********
		if (usePostInLearningPlatformPreference) {
			// Third term in eqn 28. Compute post likelihood.
			double denominator = 0;
			for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
				if (currUser.platforms[p] == 0) {
					continue;
				}
				denominator += Math.exp(x[p]);

			}
			double logDenominator = Math.log(denominator);
			for (int s = 0; s < currUser.nPosts; s++) {
				if (currUser.postBatches[s] == batch) {
					int z = currUser.posts[s].topic;
					int p = currUser.posts[s].platform;
					if (z == k) {
						postLikelihood += x[p] - logDenominator;
					}
				}
			}
		}

		if (usePrior) {
			// Fourth term in eqn 28. Compute platform likelihood.
			for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
				platformPreferencePrior += ((alpha - 1) * Math.log(x[p])) - (x[p] / theta);
			}
		}
		likelihood = linkLikelihood + nonLinkLikelihood + postLikelihood + platformPreferencePrior;
		// likelihood = postLikelihood + platformLikelihood;
		// likelihood = linkLikelihood + nonLinkLikelihood;

		return likelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to platform
	 * preference of u in topic k when the preference is x, i.e., if if
	 * L(data|parameters) = f(Eta_uk) + const-of-Eta_uk then this function
	 * return df/dEta_ukp at Eta_ukp = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private static double gradLikelihood_platformPreference(int u, int k, int j, double x) {
		// Refer to Eqn 31 in Learning paper
		double linkLikelihood = 0;
		double nonLinkLikelihood = 0;
		double postLikelihood = 0;
		double platformPreferencePrior = 0;
		double likelihood = 0;

		// Set the current user to be
		User currUser = dataset.users[u];

		double[] tempExps = new double[Configure.NUM_OF_PLATFORM];
		double sumExp = 0;
		for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
			if (currUser.platforms[p] == 0) {
				continue;
			}
			if (p == j) {
				tempExps[p] += Math.exp(x);
			} else {
				tempExps[p] = Math.exp(currUser.topicalPlatformPreference[k][p]);
			}
			sumExp += tempExps[p];
		}
		double sumExpSqr = sumExp * sumExp;
		double[] topicalRelativePlatformPreference = new double[Configure.NUM_OF_PLATFORM];
		for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
			if (currUser.platforms[p] == 0) {
				topicalRelativePlatformPreference[p] = 0;
			} else {
				topicalRelativePlatformPreference[p] = tempExps[p] / sumExp;
			}
		}

		// ****** link part ********
		if (useLinkInLearningPlatformPreference) {
			double tempExpHA;
			double tempGrad;
			// First term in eqn 31. Compute link likelihood.
			if (currUser.followings != null) {
				for (int i = 0; i < currUser.followings.length; i++) {
					int v = currUser.followings[i].followingIndex;
					User following = dataset.users[v];
					int p = currUser.followings[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += currUser.hubs[z] * topicalRelativePlatformPreference[p] * following.authorities[z]
									* following.topicalRelativePlatformPreference[z][p];
						} else {
							HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
									* following.authorities[z] * following.topicalRelativePlatformPreference[z][p];
						}
					}
					HupAvp = HupAvp * lamda;
					tempExpHA = Math.exp(-HupAvp);

					if (p == j) {
						tempGrad = tempExps[p] * sumExp - tempExps[p] * tempExps[j];
					} else {
						tempGrad = -tempExps[p] * tempExps[j];
					}
					tempGrad /= sumExpSqr;
					linkLikelihood += ((1 / (1 - tempExpHA)) * (-tempExpHA)
							* (-lamda * currUser.hubs[k] * following.authorities[k]
									* following.topicalRelativePlatformPreference[k][p] * tempGrad))
							- ((1 / (tempExpHA + 1)) * tempExpHA * (-lamda * currUser.hubs[k] * following.authorities[k]
									* following.topicalRelativePlatformPreference[k][p] * tempGrad));

				}
			}
			if (currUser.followers != null) {
				for (int i = 0; i < currUser.followers.length; i++) {
					int v = currUser.followers[i].followerIndex;
					User follower = dataset.users[v];
					int p = currUser.followers[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * topicalRelativePlatformPreference[p];
						} else {
							HupAvp += follower.hubs[z] * follower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];
						}
					}
					HupAvp = HupAvp * lamda;
					tempExpHA = Math.exp(-HupAvp);

					if (p == j) {
						tempGrad = tempExps[p] * sumExp - tempExps[p] * tempExps[j];
					} else {
						tempGrad = -tempExps[p] * tempExps[j];
					}
					tempGrad /= sumExpSqr;

					linkLikelihood += ((1 / (1 - tempExpHA)) * (-tempExpHA)
							* (-lamda * follower.hubs[k] * follower.topicalRelativePlatformPreference[k][p]
									* currUser.authorities[k] * tempGrad))
							- (1 / (tempExpHA + 1) * tempExpHA
									* (-lamda * follower.hubs[k] * follower.topicalRelativePlatformPreference[k][p]
											* currUser.authorities[k] * tempGrad));

				}
			}

			// Second term in eqn 31. Compute non link likelihood.
			if (currUser.nonFollowings != null) {
				for (int i = 0; i < currUser.nonFollowings.length; i++) {
					int v = currUser.nonFollowings[i].followingIndex;
					User nonFollowing = dataset.users[v];
					int p = currUser.nonFollowings[i].platform;

					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += currUser.hubs[z] * topicalRelativePlatformPreference[p]
									* nonFollowing.authorities[z]
									* nonFollowing.topicalRelativePlatformPreference[z][p];
						} else {
							HupAvp += currUser.hubs[z] * currUser.topicalRelativePlatformPreference[z][p]
									* nonFollowing.authorities[z]
									* nonFollowing.topicalRelativePlatformPreference[z][p];
						}

					}
					HupAvp = HupAvp * lamda;
					tempExpHA = Math.exp(-HupAvp);

					if (p == j) {
						tempGrad = tempExps[p] * sumExp - tempExps[p] * tempExps[j];
					} else {
						tempGrad = -tempExps[p] * tempExps[j];
					}
					tempGrad /= sumExpSqr;

					nonLinkLikelihood += (-lamda * currUser.hubs[k] * nonFollowing.authorities[k]
							* nonFollowing.topicalRelativePlatformPreference[k][p] * tempGrad)
							- ((1 / (tempExpHA + 1)) * tempExpHA
									* (-lamda * currUser.hubs[k] * nonFollowing.authorities[k]
											* nonFollowing.topicalRelativePlatformPreference[k][p] * tempGrad));

				}
			}
			if (currUser.nonFollowers != null) {
				for (int i = 0; i < currUser.nonFollowers.length; i++) {
					int v = currUser.nonFollowers[i].followerIndex;
					User nonFollower = dataset.users[v];
					int p = currUser.nonFollowers[i].platform;
					// Compute H_u^p * A_v^p
					double HupAvp = 0;
					for (int z = 0; z < nTopics; z++) {
						if (z == k) {
							HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * topicalRelativePlatformPreference[p];
						} else {
							HupAvp += nonFollower.hubs[z] * nonFollower.topicalRelativePlatformPreference[z][p]
									* currUser.authorities[z] * currUser.topicalRelativePlatformPreference[z][p];
						}

					}
					HupAvp = HupAvp * lamda;
					tempExpHA = Math.exp(-HupAvp);
					if (p == j) {
						tempGrad = tempExps[p] * sumExp - tempExps[p] * tempExps[j];
					} else {
						tempGrad = -tempExps[p] * tempExps[j];
					}
					tempGrad /= sumExpSqr;

					nonLinkLikelihood += (-lamda * nonFollower.hubs[k]
							* nonFollower.topicalRelativePlatformPreference[k][p] * currUser.authorities[k] * tempGrad)
							- ((1 / (tempExpHA + 1)) * tempExpHA
									* (-lamda * nonFollower.hubs[k]
											* nonFollower.topicalRelativePlatformPreference[k][p]
											* currUser.authorities[k] * tempGrad));
				}

			}
		}
		// ***** post part ******
		if (usePostInLearningPlatformPreference) {
			// Third term in eqn 31. Compute post likelihood.
			double firstSubTerm = 0;
			double secondSubTerm = 0;

			for (int s = 0; s < currUser.nPosts; s++) {
				int z = currUser.posts[s].topic;
				int p = currUser.posts[s].platform;
				if (currUser.postBatches[s] == batch) {
					if (z == k && j == p) {
						firstSubTerm += 1;
					}
					if (z == k) {
						secondSubTerm += tempExps[j] / sumExp;
					}
				}
			}
			postLikelihood += firstSubTerm - secondSubTerm;
		}
		if (usePrior) {
			// Fourth term in eqn 28. Compute platform likelihood.
			platformPreferencePrior += ((alpha - 1) / x) - (1 / theta);
		}

		likelihood = linkLikelihood + nonLinkLikelihood + postLikelihood + platformPreferencePrior;
		// likelihood = postLikelihood + platformLikelihood;
		// likelihood = linkLikelihood + nonLinkLikelihood;

		return likelihood;

	}

	/***
	 * alternating step to optimize platform preference of user u
	 * 
	 * @param u
	 */
	private static void altOptimize_PlatformPreference(int u, int k) {
		// the topical platform preferences is a 2D array

		double[] grad = new double[Configure.NUM_OF_PLATFORM];
		double[] currentX = dataset.users[u].topicalPlatformPreference[k];
		double[] x = new double[Configure.NUM_OF_PLATFORM];

		double currentF = 0 - getLikelihood_platformPreference(u, k, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;
		boolean isChanged = false;

		for (int iter = 0; iter < maxIteration_platformPreference; iter++) {
			// compute gradient
			for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
				if (dataset.users[u].platforms[p] == 0) {
					grad[p] = 0;
				} else {
					grad[p] = 0 - gradLikelihood_platformPreference(u, k, p, currentX[p]);
				}
			}

			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;
			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					if (dataset.users[u].platforms[p] == 0) {
						x[p] = Double.NEGATIVE_INFINITY;
					} else {
						x[p] = currentX[p] - lineSearch_lambda * grad[p];
						if (x[p] < epsilon) {
							x[p] = epsilon;
						}
					}
				}
				// compute f at the new x
				f = 0 - getLikelihood_platformPreference(u, k, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					diff += Math.pow(currentX[p] - x[p], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					currentX[p] = x[p];
				}
				isChanged = true;
				// to see if F actually reduce after every iteration
				if (opt_platform_verbose) {
					System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u, iter, f);
				}
			} else {
				// to see if F actually reduce after every iteration
				if (opt_platform_verbose) {
					System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u, iter, f);
				}
				break;// cannot improve further
			}
		}
		if (isChanged) {
			dataset.users[u].topicalRelativePlatformPreference[k] = MathTool
					.softmax(dataset.users[u].topicalPlatformPreference[k]);
		}
	}

	/***
	 * to sample topic for post n of user u
	 * 
	 * @param u
	 * @param n
	 */
	private static void samplePostTopic_EMGibbs(int u, int n) {
		// Refer to Eqn 32 in Learning paper

		// Set the current user to be u
		User currUser = dataset.users[u];

		double sump = 0;
		// p: p(z_u,s = z| rest)

		double[] p = new double[nTopics];
		double max = -Double.MAX_VALUE;
		for (int z = 0; z < nTopics; z++) {
			// User-topic
			p[z] = currUser.topicalInterests[z];

			// topic-word
			Post currPost = currUser.posts[n];
			for (int w = 0; w < currPost.words.length; w++) {
				int word = currPost.words[w];
				p[z] += Math.log(topicWordDist[z][word]);
			}

			// preference
			p[z] += currUser.topicalPlatformPreference[z][currPost.platform];

			// update min
			if (max < p[z]) {
				max = p[z];
			}

		}
		double[] plog = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			plog[z] = p[z];
		}
		// convert log(sump) to probability
		for (int z = 0; z < nTopics; z++) {
			p[z] = p[z] - max;
			p[z] = Math.exp(p[z]);

			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}

		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z]) {
				continue;
			} else {
				// Sample topic
				currUser.posts[n].topic = z;
				/*
				 * if (currUser.topicalInterests[z] <= 10E-12) {
				 * System.err.println("Something wrong!!! "); for (int k = 0; k
				 * < nTopics; k++) { System.out.printf(
				 * "theta[%d] = %.12f \tlog(theta[%d]) = %.12f \tplog[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n"
				 * , k, currUser.topicalInterests[k], k,
				 * Math.log(currUser.topicalInterests[k]), k, plog[k], k, p[k],
				 * sump); } System.out.printf("z = %d\n", z); System.exit(-1); }
				 */
				return;
			}
		}
		System.err.println("Something wrong!!! ");
		for (int k = 0; k < nTopics; k++) {
			System.out.printf("theta[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n", k, currUser.topicalInterests[k], k,
					p[k], sump);
		}
		System.exit(-1);
	}

	/***
	 * alternating step to optimize topics' word distribution
	 */
	private void altOptimize_topics() {
		// initialize the count variables
		for (int k = 0; k < nTopics; k++) {
			sum_nzw[k] = 0;
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				n_zw[k][w] = 0;
			}
		}

		// update count variable base on the post topic assigned
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int n = 0; n < currUser.posts.length; n++) {
				Post currPost = currUser.posts[n];
				// only consider posts in batch
				if (currUser.postBatches[n] == batch) {
					int z = currPost.topic;
					for (int i = 0; i < currPost.nWords; i++) {
						int wordIndex = currPost.words[i];
						sum_nzw[z] += 1;
						n_zw[z][wordIndex] += 1;
					}
				}
			}
		}

		// compute topic word distribution
		for (int k = 0; k < nTopics; k++) {
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				topicWordDist[k][w] = (n_zw[k][w] + gamma) / (sum_nzw[k] + gamma * dataset.vocabulary.length);
			}
		}
	}

	/***
	 * checking if the gradient computation of likelihood of user topical
	 * interest X_{u,k} is properly implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_TopicalInterest(int u, int k) {
		double DELTA = 1;

		double[] x = dataset.users[u].topicalInterests;

		double f = getLikelihood_topicalInterest(u, x);
		double g = gradLikelihood_topicalInterest(u, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_topicalInterest(u, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(String.format("[TopicInterest] u = %d k = %d DELTA = %.12f numGrad = %f grad = %f\n", u,
					k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;
		}
	}

	/***
	 * checking if the gradient computation of likelihood of A_{v,k} is properly
	 * implemented
	 * 
	 * @param v
	 * @param k
	 */
	public void gradCheck_Authority(int v, int k) {
		double DELTA = 1;

		double[] x = dataset.users[v].authorities;
		double f = getLikelihood_authority(v, x);
		double g = gradLikelihood_authority(v, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_authority(v, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(String.format("[Authority] v= %d k = %d DELTA = %f numGrad = %f grad = %f\n", v, k, DELTA,
					numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;
		}
	}

	/***
	 * checking if the gradient computation of likelihood of H_{u,k} is properly
	 * implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_Hub(int u, int k) {
		double DELTA = 1;

		double[] x = dataset.users[u].hubs;

		double f = getLikelihood_hub(u, x);
		double g = gradLikelihood_hub(u, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_hub(u, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(
					String.format("[Hub] u = %d k = %d DELTA = %f numGrad = %f grad = %f\n", u, k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;

		}
	}

	/***
	 * checking if the gradient computation of likelihood of H_{u,k} is properly
	 * implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_PlatformPreference(int u, int k, int p) {
		if (dataset.users[u].platforms[p] == 0) {
			System.out.printf("user %d is not active on platform %d\n", u, p);
			System.exit(-1);
		}

		double DELTA = 1;

		double[] x = new double[Configure.NUM_OF_PLATFORM];
		for (int i = 0; i < Configure.NUM_OF_PLATFORM; i++) {
			x[i] = dataset.users[u].topicalPlatformPreference[k][i];
		}

		double f = getLikelihood_platformPreference(u, k, x);
		double g = gradLikelihood_platformPreference(u, k, p, x[p]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[p] += DELTA;
			double DELTAF = getLikelihood_platformPreference(u, k, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(
					String.format("[Hub] u = %d k = %d DELTA = %f numGrad = %f grad = %f\n", u, k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[p] -= DELTA;

		}
	}

	private void samplePostTopic_Gibbs(int u, int n) {
		Post currPost = dataset.users[u].posts[n];
		// reduce current counts
		int currZ = currPost.topic;
		n_zu[currZ][u]--;
		if (n_zu[currZ][u] < 0) {
			System.out.printf("u = %d z = %d n_zu = %d\n", u, currZ, n_zu[currZ][u]);
			System.exit(-1);
		}
		sum_nzw[currZ] -= currPost.nWords;
		for (int w = 0; w < currPost.nWords; w++) {
			int word = currPost.words[w];
			n_zw[currZ][word]--;
		}

		double sump = 0;
		// p: p(z_u,s = z| rest)

		double[] p = new double[nTopics];
		double max = -Double.MAX_VALUE;
		for (int z = 0; z < nTopics; z++) {
			// User-topic
			p[z] = Math.log(n_zu[z][u] + alpha);
			// topic-word
			for (int w = 0; w < currPost.nWords; w++) {
				int word = currPost.words[w];
				p[z] += Math.log((n_zw[z][word] + gamma) / (sum_nzw[z] + gamma * dataset.vocabulary.length));
			}
			// update min
			if (max < p[z]) {
				max = p[z];
			}

		}
		// convert log(sump) to probability
		for (int z = 0; z < nTopics; z++) {
			p[z] = p[z] - max;
			p[z] = Math.exp(p[z]);
			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}

		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z]) {
				continue;
			} else {
				// Sample topic
				currPost.topic = z;
				// increase the counts
				n_zu[z][u]++;
				sum_nzw[z] += currPost.nWords;
				for (int w = 0; w < currPost.nWords; w++) {
					int word = currPost.words[w];
					n_zw[z][word]++;
				}
				return;
			}
		}
		System.err.println("Something wrong!!! ");
		for (int k = 0; k < nTopics; k++) {
			System.out.printf("theta[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n", k,
					dataset.users[u].topicalInterests[k], k, p[k], sump);
		}
		System.exit(-1);
	}

	/****
	 * initialize topic assignment for posts of user u
	 * 
	 * @param u
	 */
	private static void initPostTopic(int u) {
		// System.out.printf("initializing for user %d\n", u);
		User currUser = dataset.users[u];
		for (int n = 0; n < currUser.posts.length; n++) {
			// only consider posts in batch
			if (currUser.postBatches[n] == batch) {
				int randTopic = rand.nextInt(nTopics);
				currUser.posts[n].topic = randTopic;
			}
		}
	}

	private void gibbsInit() {
		// initialize the count variables
		int[][] final_n_zw = new int[nTopics][dataset.vocabulary.length];
		int[] final_sum_nzw = new int[nTopics];
		int[][] final_n_zu = new int[nTopics][dataset.nUsers];

		for (int z = 0; z < nTopics; z++) {
			sum_nzw[z] = 0;
			final_sum_nzw[z] = 0;
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				n_zw[z][w] = 0;
				final_n_zw[z][w] = 0;
			}
			for (int u = 0; u < dataset.nUsers; u++) {
				n_zu[z][u] = 0;
				final_n_zu[z][u] = 0;
			}
		}
		// update count variable base on the posts' topic assignment
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int n = 0; n < currUser.posts.length; n++) {
				Post currPost = currUser.posts[n];
				// only consider posts in batch
				if (currUser.postBatches[n] == batch) {
					int z = currPost.topic;
					n_zu[z][u]++;
					sum_nzw[z] += currPost.nWords;
					for (int w = 0; w < currPost.nWords; w++) {
						int wordIndex = currPost.words[w];
						n_zw[z][wordIndex]++;
					}

				}
			}
		}
		// gibss sampling
		for (int iter = 0; iter < gibbs_BurningPeriods + max_Gibbs_Iterations; iter++) {
			System.out.println("Gibb Iteration:" + iter);
			for (int u = 0; u < dataset.nUsers; u++) {
				for (int n = 0; n < dataset.users[u].nPosts; n++) {
					if (dataset.users[u].postBatches[n] == batch) {
						samplePostTopic_Gibbs(u, n);
					}
				}
			}

			if (iter < gibbs_BurningPeriods) {
				continue;
			}
			if (iter % gibbs_Sampling_Gap != 0) {
				continue;
			}

			for (int z = 0; z < nTopics; z++) {
				final_sum_nzw[z] += sum_nzw[z];
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					final_n_zw[z][w] += n_zw[z][w];
				}
				for (int u = 0; u < dataset.nUsers; u++) {
					final_n_zu[z][u] += n_zu[z][u];
				}
			}
		}

		int[] final_sum_nzu = new int[dataset.nUsers];
		for (int u = 0; u < dataset.nUsers; u++) {
			final_sum_nzu[u] = 0;
		}

		// topics
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				topicWordDist[z][w] = (final_n_zw[z][w] + gamma)
						/ (final_sum_nzw[z] + gamma * dataset.vocabulary.length);
			}
			for (int u = 0; u < dataset.nUsers; u++) {
				final_sum_nzu[u] += final_n_zu[z][u];
			}
		}
		// users' topical interests
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int z = 0; z < nTopics; z++) {
				dataset.users[u].topicalInterests[z] = (final_n_zu[z][u] + alpha)
						/ (final_sum_nzu[u] + nTopics * alpha);

				if (dataset.users[u].topicalInterests[z] < 0) {
					System.out.printf("u = %d z = %d theta = %f\n", u, z, dataset.users[u].topicalInterests[z]);
					System.exit(-1);
				}
			}
		}
	}

	private void initByGroundTruth(String groundtruthPath) {
		dataset.getGroundTruth(groundtruthPath, nTopics);
		for (int u = 0; u < dataset.users.length; u++) {
			for (int i = 0; i < dataset.users[u].nPosts; i++) {
				dataset.users[u].posts[i].topic = dataset.users[u].posts[i].groundTruthTopic;
			}
			for (int z = 0; z < nTopics; z++) {
				dataset.users[u].topicalInterests[z] = dataset.users[u].groundtruth_TopicalInterests[z];
				dataset.users[u].authorities[z] = dataset.users[u].groundtruth_Authorities[z];
				dataset.users[u].hubs[z] = dataset.users[u].groundtruth_Hubs[z];
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					dataset.users[u].topicalPlatformPreference[z][p] = dataset.users[u].groundtruth_TopicalPlatformPreference[z][p];
				}
				dataset.users[u].topicalRelativePlatformPreference[z] = MathTool
						.softmax(dataset.users[u].topicalPlatformPreference[z]);
			}
		}
		altOptimize_topics();
	}

	private void initPlatformPreferenceByTopicModeling() {
		// init counts for the topical platform preferences
		int[][] userTopicPostCounts = new int[dataset.nUsers][nTopics];
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int k = 0; k < nTopics; k++) {
				userTopicPostCounts[u][k] = 0;
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					currUser.topicalPlatformPreference[k][p] = 0;
				}
			}
		}

		// update the topical platform count
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int n = 0; n < currUser.posts.length; n++) {
				Post currPost = currUser.posts[n];
				int z = currPost.topic;
				int p = currPost.platform;
				currUser.topicalPlatformPreference[z][p] += 1;
				userTopicPostCounts[u][z] += 1;
			}
		}

		// approximate user platform preference distribution
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int k = 0; k < nTopics; k++) {

				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					if (currUser.platforms[p] == 1) {
						if (userTopicPostCounts[u][k] == 0) {
							currUser.topicalPlatformPreference[k][p] = epsilon;
						} else {
							currUser.topicalPlatformPreference[k][p] = currUser.topicalPlatformPreference[k][p]
									/ userTopicPostCounts[u][k];
							if (currUser.topicalPlatformPreference[k][p] <= 0) {
								currUser.topicalPlatformPreference[k][p] = epsilon;
							}
						}
					} else {
						currUser.topicalPlatformPreference[k][p] = Double.NEGATIVE_INFINITY;
					}
				}
			}
		}

		// for (int u = 0; u < dataset.nUsers; u++) {
		// User currUser = dataset.users[u];
		// for (int k=0; k<nTopics; k++){
		// for (int p=0; p<Configure.NUM_OF_PLATFORM; p++){
		// System.out.println(u+","+k+","+p+","+currUser.topicalPlatformPreference[k][p]);
		//
		// }
		//
		// }
		// }

	}

	/***
	 * initialize the data before training
	 */
	private void init() {
		// alpha = (double) (20) / (double) (nTopics);// prior for users'
		// interest
		gamma = 0.001;// prior for word distribution
		sigma = 2.0;// shape parameter of users' authorities
		delta = 2.0;// shape parameter of users' hubs
		kappa = 2.0;// shape parameter of user interest latent vector
		alpha = 2.0;// shape parameter of user platform preference vector
		theta = 0.5;// scale parameter of user interests/platform preference
		// vectors
		rand = new Random(1);

		// allocate memory for counts
		n_zu = new int[nTopics][dataset.nUsers];
		n_zw = new int[nTopics][dataset.vocabulary.length];
		sum_nzw = new int[nTopics];

		// allocate memory for the users
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			currUser.authorities = new double[nTopics];
			currUser.hubs = new double[nTopics];
			currUser.topicalInterests = new double[nTopics];
			currUser.topicalPlatformPreference = new double[nTopics][Configure.NUM_OF_PLATFORM];
			currUser.optAuthorities = new double[nTopics];
			currUser.optHubs = new double[nTopics];
			currUser.optTopicalInterests = new double[nTopics];
			currUser.optTopicalPlatformPreference = new double[nTopics][Configure.NUM_OF_PLATFORM];
			currUser.topicalRelativePlatformPreference = new double[nTopics][];

		}

		// allocate memory for topics
		topicWordDist = new double[nTopics][dataset.vocabulary.length];
		optTopicWordDist = new double[nTopics][dataset.vocabulary.length];

		// initialize the count variables
		for (int u = 0; u < dataset.nUsers; u++) {
			sum_nzu[u] = 0;
			for (int k = 0; k < nTopics; k++) {
				n_zu[k][u] = 0;
			}
		}

		if (initByGroundTruth) {
			initByGroundTruth(datapath);
			return;
		}

		// init topic assignment for posts
		ExecutorService executor = Executors.newFixedThreadPool(nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "initPostTopic");
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
			// do nothing, just wait for the threads to finish
		}

		if (initByTopicModeling) {
			// initialize by topic modeling
			gibbsInit();

			if (InitPlatformPreferenceByTopicModeling) {
				initPlatformPreferenceByTopicModeling();
			}

			if (userGlobalMin) {
				// Update the global min for user topical interests
				getGlobalTopicInterestMin();
			}

			// init users' interest, platform preference, authority, and hub
			executor = Executors.newFixedThreadPool(nParallelThreads);
			for (int i = 0; i < nParallelThreads; i++) {
				Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "gibbsInitUser");
				executor.execute(worker);
			}
			executor.shutdown();
			while (!executor.isTerminated()) {
				// do nothing, just wait for the threads to finish
			}
		} else {
			// initialize by alternating optimizing
			altOptimize_topics();
			// init users' interest, platform preference, authority, and hub
			executor = Executors.newFixedThreadPool(nParallelThreads);
			for (int i = 0; i < nParallelThreads; i++) {
				Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "randomInitUser");
				executor.execute(worker);
			}
			executor.shutdown();
			while (!executor.isTerminated()) {
				// do nothing, just wait for the threads to finish
			}
		}

	}

	public void altCheck_TopicalInterest(int u) {
		altOptimize_topicalInterest(u);
	}

	public void altCheck_Authority(int u) {
		altOptimize_Authorities(u);
	}

	public void altCheck_Hub(int u) {
		altOptimize_Hubs(u);
	}

	public void altCheck_PlatformPreference(int u, int k) {
		altOptimize_PlatformPreference(u, k);
	}

	/***
	 * modeling learning
	 */
	public void train() {
		getThreadIndexes();
		init();

		// save initial solution
		// TopicWordsDist
		for (int z = 0; z < nTopics; z++) {
			for (int w = 0; w < dataset.vocabulary.length; w++)
				optTopicWordDist[z][w] = topicWordDist[z][w];
		}
		ExecutorService executor = null;
		// set optimized user topical interest, authority and hub
		executor = Executors.newFixedThreadPool(nParallelThreads);
		for (int i = 0; i < nParallelThreads; i++) {
			Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "updateOpt");
			executor.execute(worker);
		}
		executor.shutdown();
		while (!executor.isTerminated()) {
			// do nothing, just wait for the threads to finish
		}

		// start learning
		if (onlyLearnGibbs) {
			// output_topicWord();
			output_topicInterest();
			outputPostTopicTopWords(20);
			getOptLikelihoodPerplexity();
			getLastLikelihoodPerplexity();
			output_OptLikelihoodPerplexity();
			output_LastLikelihoodPerplexity();
			return;
		}

		double maxLikelihood = Double.NEGATIVE_INFINITY;
		double currentLikelihood = 0;
		System.out.println("Datapath:" + datapath);
		System.out.println("Alpha:" + alpha);
		System.out.println("Line Search Alpha:" + lineSearch_alpha);
		System.out.println("Line Search Beta:" + lineSearch_beta);
		System.out.println("Line Search Max Iterations:" + lineSearch_MaxIterations);
		System.out.println("#Topics:" + nTopics);
		System.out.println("#platforms:" + Configure.NUM_OF_PLATFORM);

		for (int iter = 0; iter < max_GibbsEM_Iterations; iter++) {
			// EM part that employs alternating optimization
			// topical interest
			if (learnUserInterest) {
				System.out.printf("[iter-%d] optimizing users' topical interest\n", iter);
				executor = Executors.newFixedThreadPool(nParallelThreads);
				for (int i = 0; i < nParallelThreads; i++) {
					Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "optTopicInterests");
					executor.execute(worker);
				}
				executor.shutdown();
				while (!executor.isTerminated()) {
					// do nothing, just wait for the threads to finish
				}
				System.out.printf("[iter-%d] after learning interest likelihood = %f\n", iter,
						getLikelihood_parallel());
			}
			// authority
			if (learnUserAuthority) {
				System.out.printf("[iter-%d] optimizing users' authorities\n", iter);
				executor = Executors.newFixedThreadPool(nParallelThreads);
				for (int i = 0; i < nParallelThreads; i++) {
					Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "optAuthorities");
					executor.execute(worker);
				}
				executor.shutdown();
				while (!executor.isTerminated()) {
					// do nothing, just wait for the threads to finish
				}
				System.out.printf("[iter-%d] after learning authority likelihood = %f\n", iter,
						getLikelihood_parallel());
			}
			// hub
			if (learnUserHub) {
				System.out.printf("[iter-%d] optimizing users' hubs\n", iter);
				executor = Executors.newFixedThreadPool(nParallelThreads);
				for (int i = 0; i < nParallelThreads; i++) {
					Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "optHubs");
					executor.execute(worker);
				}
				executor.shutdown();
				while (!executor.isTerminated()) {
					// do nothing, just wait for the threads to finish
				}
				System.out.printf("[iter-%d] after learning hub likelihood = %f\n", iter, getLikelihood_parallel());
			}
			// platform preference
			if (learnUserPlatformPreference) {
				System.out.printf("[iter-%d] optimizing users' platform preference\n", iter);
				if (asynchronousParallelUserPlatformPreference) {
					// TODO: check for convergence
					executor = Executors.newFixedThreadPool(nParallelThreads);
					for (int i = 0; i < nParallelThreads; i++) {
						Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i],
								"optPlatformPreferences");
						executor.execute(worker);
					}
					executor.shutdown();
					while (!executor.isTerminated()) {
						// do nothing, just wait for the threads to finish
					}
				} else {
					for (int u = 0; u < dataset.nUsers; u++) {
						for (int k = 0; k < nTopics; k++) {
							altOptimize_PlatformPreference(u, k);
						}
					}
				}
				System.out.printf("[iter-%d] after learning platform preference likelihood = %f\n", iter,
						getLikelihood_parallel());
			}

			// Gibbs part that employ topic sampling
			if (learnTopic) {
				System.out.printf("[iter-%d] optimizing topics' word distribution\n", iter);
				altOptimize_topics();
				// Gibbs part that employ topic sampling
				System.out.printf("[iter-%d] sampling topic for users' posts\n", iter);
				executor = Executors.newFixedThreadPool(nParallelThreads);
				for (int i = 0; i < nParallelThreads; i++) {
					Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "topicSample");
					executor.execute(worker);
				}
				executor.shutdown();
				while (!executor.isTerminated()) {
					// do nothing, just wait for the threads to finish
				}
			}

			// set first Likelihood as the maxLikelihood
			System.out.printf("[iter-%d] computing new likelihood\n", iter);
			currentLikelihood = getLikelihood_parallel();

			if (maxLikelihood < currentLikelihood) {
				System.out.printf("[iter-%d] saving solution\n", iter);
				maxLikelihood = currentLikelihood;
				// set optimized topicWordDist to be the current
				// TopicWordsDist
				for (int z = 0; z < nTopics; z++) {
					for (int w = 0; w < dataset.vocabulary.length; w++)
						optTopicWordDist[z][w] = topicWordDist[z][w];
				}
				// set optimized user topical interest, authority and hub
				executor = Executors.newFixedThreadPool(nParallelThreads);
				for (int i = 0; i < nParallelThreads; i++) {
					Runnable worker = new ChildThread(threadStartIndexes[i], threadEndIndexes[i], "updateOpt");
					executor.execute(worker);
				}
				executor.shutdown();
				while (!executor.isTerminated()) {
					// do nothing, just wait for the threads to finish
				}
			}
			System.out.printf("likelihood after %d steps: %f, max %f\n", iter, currentLikelihood, maxLikelihood);
			System.out.println();
		}
		// print out the learned parameters
		//output_topicWord();
		output_topicInterest();
		output_platformPreference();
		output_authority();
		output_hub();
		outputPostTopicTopWords(20);
		getOptLikelihoodPerplexity();
		getLastLikelihoodPerplexity();
		output_OptLikelihoodPerplexity();
		output_LastLikelihoodPerplexity();
		inferPostPlatform();
		outputInferedPlatform();
	}

	public void output_topicWord() {
		try {
			File f = new File(outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi + "/l_topicalWordDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int k = 0; k < nTopics; k++) {
				String text = Integer.toString(k);
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					text = text + "," + Double.toString(optTopicWordDist[k][w]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical word file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	private void outputPostTopicTopWords(int k) {
		try {
			File f = new File(outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi + "/l_topTopicWords.csv");
			BufferedWriter bw = new BufferedWriter(new FileWriter(f.getAbsoluteFile()));
			RankingTool rankTool = new RankingTool();
			WeightedElement[] topWords = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				topWords = rankTool.getTopKbyWeight(dataset.vocabulary, optTopicWordDist[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topWords[j].name + "," + topWords[j].weight + "\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out post topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_topicInterest() {
		try {
			File f = new File(
					outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi + "/l_userTopicalInterestDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optTopicalInterests[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_platformPreference() {
		try {
			File f = new File(outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi
					+ "/l_userTopicalPlatformPreferenceDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				for (int k = 0; k < nTopics; k++) {
					String text = currUser.userId + "," + k;
					for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
						text = text + "," + Double.toString(currUser.optTopicalPlatformPreference[k][p]);
					}
					fo.write(text + "\n");
				}
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to platform preference file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_authority() {
		try {
			File f = new File(outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi + "/l_userAuthorityDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optAuthorities[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to authority file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_hub() {
		try {
			File f = new File(outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi + "/l_userHubDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optHubs[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void getGlobalTopicInterestMin() {
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int k = 0; k < nTopics; k++) {
				if (globalTopicInterestsMin > currUser.topicalInterests[k]) {
					globalTopicInterestsMin = currUser.topicalInterests[k];
				}
			}
		}
		System.out.println("globalTopicInterestsMin:" + globalTopicInterestsMin);
	}

	private double getOptPostLikelihood(int u, int n) {
		// Compute likelihood of post number n of user number u
		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			p[z] = Math.log10(dataset.users[u].optTopicalInterests[z]);
			for (int i = 0; i < dataset.users[u].posts[n].nWords; i++) {
				int w = dataset.users[u].posts[n].words[i];
				p[z] += Math.log10(optTopicWordDist[z][w]);
			}
		}
		return MathTool.log10Sum(p);
	}

	private double getLastPostLikelihood(int u, int j) {
		// Compute likelihood of post number j of user number u
		double[] p = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			p[z] = Math.log10(dataset.users[u].topicalInterests[z]);
			for (int i = 0; i < dataset.users[u].posts[j].nWords; i++) {
				int w = dataset.users[u].posts[j].words[i];
				p[z] += Math.log10(topicWordDist[z][w]);
			}
		}
		return MathTool.log10Sum(p);
	}

	private void getOptLikelihoodPerplexity() {
		postOptLogLikelidhood = 0;
		postOptLogPerplexity = 0;
		int nTestPost = 0;
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int t = 0; t < dataset.users[u].nPosts; t++) {
				double logLikelihood = getOptPostLikelihood(u, t);
				if (dataset.users[u].postBatches[t] == batch)
					postOptLogLikelidhood += logLikelihood;
				else {
					postOptLogPerplexity += (-logLikelihood);
					nTestPost++;
				}
			}
		}
		postOptLogPerplexity /= nTestPost;
	}

	private void getLastLikelihoodPerplexity() {
		postLastLogLikelidhood = 0;
		postLastLogPerplexity = 0;
		int nTestPost = 0;
		for (int u = 0; u < dataset.nUsers; u++) {
			for (int t = 0; t < dataset.users[u].nPosts; t++) {
				double logLikelihood = getLastPostLikelihood(u, t);
				if (dataset.users[u].postBatches[t] == batch)
					postLastLogLikelidhood += logLikelihood;
				else {
					postLastLogPerplexity += (-logLikelihood);
					nTestPost++;
				}
			}
		}
		postLastLogPerplexity /= nTestPost;
	}

	public void output_OptLikelihoodPerplexity() {
		try {
			File f = new File(outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi + "/l_OptLikelihoodPerplexity.csv");
			FileWriter fo = new FileWriter(f);
			fo.write("PostLogLikelihood:" + postOptLogLikelidhood + "\n");
			fo.write("PostLogPerplexity:" + postOptLogPerplexity + "\n");
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}

	public void output_LastLikelihoodPerplexity() {
		try {
			File f = new File(outputPath + "/" + nTopics + "/omega_" + omega + "_phi_" + phi + "/l_LastLikelihoodPerplexity.csv");
			FileWriter fo = new FileWriter(f);
			fo.write("PostLogLikelihood:" + postLastLogLikelidhood + "\n");
			fo.write("PostLogPerplexity:" + postLastLogPerplexity + "\n");
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	private void inferPostPlatform() {
		for (int u = 0; u < dataset.users.length; u++) {
			for (int j = 0; j < dataset.users[u].posts.length; j++) {
				if (dataset.users[u].postBatches[j] == batch)
					continue;
				int z = dataset.users[u].posts[j].topic;
				double[] prp = MathTool
						.softmax(dataset.users[u].optTopicalPlatformPreference[z]);
				
				if (prp[0] > prp[1]){
					dataset.users[u].posts[j].inferedPlatform = 0;
				} else{
					dataset.users[u].posts[j].inferedPlatform = 1;
				}
				
//				double maxLikelihood = getPostLikelihood(u, j, 0, 0);
//				dataset.users[u].posts[j].inferedPlatform = 0;
//				for (int p = 0; p < nPlatforms; p++) {
//					for (int z = 0; z < nTopics; z++) {
//						double likeLihood = getPostLikelihood(u, j, p, z);
//						if (maxLikelihood < likeLihood) {
//							maxLikelihood = likeLihood;
//							dataset.users[u].posts[j].inferedPlatform = p;
//						}
//					}
//				}
			}
		}
	}
	
	private void outputInferedPlatform() {
		try {
			SystemTool.createFolder(outputPath+ "/" + nTopics + "/omega_" + omega + "_phi_" + phi +"/", "inferedPlatforms");
			for (int u = 0; u < dataset.users.length; u++) {
				//String filename = outputPath + SystemTool.pathSeparator + "inferedPlatforms" + SystemTool.pathSeparator
				//		+ dataset.users[u].userId + ".csv";
				File f = new File(outputPath+ "/" + nTopics + "/omega_" + omega + "_phi_" + phi +"/", "inferedPlatforms"+"/"
						+dataset.users[u].userId + ".csv");
				FileWriter fo = new FileWriter(f);
				//BufferedWriter bw = new BufferedWriter(new FileWriter(filename));
				for (int j = 0; j < dataset.users[u].posts.length; j++) {
					if (dataset.users[u].postBatches[j] == batch)
						continue;
					fo.write(dataset.users[u].posts[j].postId + "," + dataset.users[u].posts[j].inferedPlatform + "," +  dataset.users[u].posts[j].platform + "\n");
				}
				fo.close();
			}
		} catch (Exception e) {
			System.out.println("Error in writing out post topics to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	public static void main(String[] args) {
		//String datasetPath = "E:/code/java/MP-HAT/mp-hat/syn_data/";
		// String datasetPath =
		// "/Users/roylee/Documents/Chardonnay/mp-hat/syn_data/";
		//String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/balance";
		//String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/balance";
		//String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/balance_2";
		//String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/balance_2/mp_subset";
		// String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/balance/instagram";
		// String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/hat_data/instagram";
		// String datasetPath = "E:/users/roylee.2013/MP-HAT/mp-hat/data/combined";
		// String datasetPath = "E:/users/roylee.2013/MP-HAT/mp-hat/syn_data";
		//String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/combined";
		String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/balance_2/syn_skewed";
		//String datasetPath = "F:/users/roylee/MP-HAT/mp-hat/data/balance_2/syn_uniform";
		int nTopics = 10;
		int batch = 1;
		MultiThreadMPHAT model = new MultiThreadMPHAT(datasetPath, nTopics, batch, datasetPath);
		
		//nTopics = 20;
		//model = new MultiThreadMPHAT(datasetPath, nTopics, batch);

		// model.getThreadIndexes();
		// model.init();

		// System.out.printf("%f \t\t %f\n", getLikelihood(),
		// getLikelihood_parallel());

		// Random rand = new Random();

		// int u = rand.nextInt(100);
		// int k = rand.nextInt(nTopics);

		// initByTopicModeling = true;

		// u = 1;
		// model.altCheck_TopicalInterest(u);
		// model.altCheck_Authority(u);
		// model.altCheck_Hub(u);
		// model.altCheck_PlatformPreference(u, k);
		long startTime = System.currentTimeMillis();
		model.train();
		long estimatedTime = System.currentTimeMillis() - startTime;
		System.out.println(nParallelThreads+","+estimatedTime);
		
		// model.gradCheck_Authority(u, k);
		// model.gradCheck_Hub(u, k);
		// model.gradCheck_TopicalInterest(u, k);
		// model.gradCheck_PlatformPreference(u, k, 1);
	}

}
