package model;

import tool.*;

import java.io.BufferedWriter;
import java.util.Arrays;
import java.util.Random;
import org.apache.commons.math3.distribution.NormalDistribution;
import org.apache.commons.math3.distribution.GammaDistribution;
import java.io.File;
import java.io.FileWriter;

public class MPHAT {
	public String datapath;
	public Dataset dataset;
	public int nTopics;
	public int batch;

	// priors
	public double alpha;// prior for users' platform preferences
	public double kappa;// prior for user's topic interests
	public double theta;
	public double sigma;// variance of users' authorities
	public double delta;// variance of users' hubs
	public double gamma; // variance of topic word distribution
	public double epsilon = 0.000001;
	public double lamda = 0.001;

	public Random rand;

	// Gibbs sampling variables
	// user-topic counts
	public int[][] n_zu = null; // n_zu[z][u]: number of times topic z is
	// observed in posts by user u
	public int[] sum_nzu = null; // sum_nzu[u] total number of topics that are
	// observed in posts by user u

	// topic-word counts
	public int[][] n_zw = null; // n_wz[z][w]: number of times word w is
	// generated by topic z in a post
	public int[] sum_nzw = null; // sum_nwz[z]: total number of times words that
	// are generated by topic z in a post

	// topic-word distribution
	public double[][] topicWordDist = null; // topicWordDist[k][w]: the
	// distribution of word w for topic
	// k. Sum of each words distribution
	// for each k = 1. This tau in our
	// paper

	public double[][] optTopicWordDist = null; // optimized topicWordDist[k][w]

	// options for learning
	public double lineSearch_alpha = 0.0001;
	public double lineSearch_beta = 0.1;
	public int lineSearch_MaxIterations = 10;;
	public double lineSearch_lambda;

	public int maxIteration_topicalInterest = 10;
	public int maxIteration_Authorities = 10;
	public int maxIteration_Hubs = 10;
	public int maxIteration_platformPreference = 10;

	public int max_GibbsEM_Iterations = 500;

	private static Configure.ModelMode mode;

	/***
	 * 
	 * @param _datasetPath
	 * @param _nTopics
	 */
	public MPHAT(String _datasetPath, int _nTopics, int _batch) {
		this.datapath = _datasetPath;
		this.dataset = new Dataset(_datasetPath, _batch, false);
		this.nTopics = _nTopics;
		this.batch = _batch;
		n_zu = new int[nTopics][dataset.nUsers];
		sum_nzu = new int[dataset.nUsers];
		n_zw = new int[nTopics][dataset.vocabulary.length];
		sum_nzw = new int[nTopics];
		topicWordDist = new double[nTopics][dataset.vocabulary.length];
	}

	/***
	 * get likelihood of the whole dataset
	 * 
	 * @return
	 */
	private double getLikelihood() {
		// to be written
		// Compute the likelihood to make sure that it is improving L(text) +
		// L(link)
		// value can be more than 1
		// sum of eqn 1 -10
		return 0;

	}

	/***
	 * compute likelihood of data as a function of topical interest of u when
	 * the interest is x, i.e., if L(data|parameters) = f(X_u) + const-of-X_u
	 * then this function returns f(x)
	 * 
	 * @param u
	 * @return
	 */
	private double getLikelihood_topicalInterest(int u, double[] x) {
		// Refer to Eqn 16 in Learning paper for Formula

		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double postLikelihood = 0;
		double topicLikelihood = 0;
		double finalLikelihood = 0;
		double denominator = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		for (int k = 0; k < nTopics; k++) {
			// First term in eqn 16
			hubLikelihood += -((currUser.hubs[k] * delta) / x[k]) - (delta * Math.log(x[k]));

			// Second term in eqn 16
			authorityLikelihood += -((currUser.authorities[k] * sigma) / x[k]) - (sigma * Math.log(x[k]));

			// Fourth term in eqn 16
			topicLikelihood += ((kappa - 1) * Math.log(x[k])) - (x[k] / theta);

			// denominator of third term in eqn 16
			denominator += Math.exp(x[k]);

		}

		for (int i = 0; i < currUser.nPosts; i++) {
			// Only compute post likelihood of posts which are in batch (i.e.
			// training batch = 1)
			if (currUser.postBatches[i] == batch) {

				// Third term in eqn 16
				int postTopic = currUser.posts[i].topic;
				postLikelihood += Math.log(Math.exp(x[postTopic]) / denominator);

			}
		}
		finalLikelihood = authorityLikelihood + hubLikelihood + postLikelihood + topicLikelihood;

		return finalLikelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to interest of u in
	 * topic k when the interest is x, i.e., if if L(data|parameters) = f(X_u) +
	 * const-of-X_u then this function return df/dX_uk at X_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_topicalInterest(int u, int k, double x) {
		// Refer to Eqn 18 in Learning paper
		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double postLikelihood = 0;
		double topicLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		// First term in eqn 18
		hubLikelihood = ((currUser.hubs[k] * delta) / Math.pow(x, 2)) - (delta / x);

		// Second term in eqn 18
		authorityLikelihood = ((currUser.authorities[k] * sigma) / Math.pow(x, 2)) - (sigma / x);

		// Third term in eqn 18
		double first_sub_term = 0;
		double second_sub_term = 0;
		double denominator = 0;
		for (int z = 0; z < nTopics; z++) {
			if (z == k) {
				denominator += Math.exp(x);
			} else {
				denominator += Math.exp(currUser.topicalInterests[z]);
			}
		}
		for (int i = 0; i < currUser.nPosts; i++) {
			// Only compute post likelihood of posts which are in batch (i.e.
			// training batch = 1)
			int currPostTopic = currUser.posts[i].topic;

			if (currUser.postBatches[i] == batch) {
				// Only consider posts which are assigned topic k (i.e. z_{v,s}
				// = k)

				if (currUser.posts[i].topic == k) {
					first_sub_term++;
				}
				second_sub_term += (1 / denominator) * Math.exp(x);
			}

		}
		postLikelihood = first_sub_term - second_sub_term;

		// Fourth term in eqn 18
		topicLikelihood = ((kappa - 1) / x) - (1 / theta);

		gradLikelihood = authorityLikelihood + hubLikelihood + postLikelihood + topicLikelihood;

		return gradLikelihood;

	}

	/***
	 * alternating step to optimize topical interest of u
	 * 
	 * @param u
	 */
	private void altOptimize_topicalInterest(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].topicalInterests;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_topicalInterest(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		for (int iter = 0; iter < maxIteration_topicalInterest; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_topicalInterest(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
				}

				// x = tool.MathTool.softmax(x);
				// x = simplexProjection(x, 1);

				// this step to make sure that we compute f at the new x
				f = 0 - getLikelihood_topicalInterest(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_topic: u = %d iter = %d f = %f\n", u,
				// iter, f);
			} else {
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_topic: u = %d iter = %d f = %f\n", u,
				// iter, f);
				break;// cannot improve further
			}
		}
	}

	/***
	 * compute likelihood of data as a function of authority of u when the
	 * authority is x, i.e., if L(data|parameters) = f(A_u) + const-of-A_u then
	 * this function returns f(x)
	 * 
	 * @param v
	 * @param x[]
	 * @return
	 */
	private double getLikelihood_authority(int v, double[] x) {
		// Refer to Eqn 24 in Learning paper
		double followerLikelihood = 0;
		double nonFollowerLikelihood = 0;
		double postLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[v];

		// First term in eqn 24. Compute follower likelihood.
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int u = currUser.followers[i].followerIndex;
					User follower = dataset.users[u];
					int followerPlatform = currUser.followers[i].platform;

					// only consider this user if he exist in the platform and
					// follower relationships in the platform
					if (currUser.platforms[p] == 1 && followerPlatform == p) {
						// Compute H_u^p * A_v^p
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							HupAvp += follower.hubs[z] * follower.topicalPlatformPreference[z][p] * x[z]
									* currUser.topicalPlatformPreference[z][p];// now
							// A_v
							// is
							// x
						}
						HupAvp = HupAvp * lamda;
						double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
						followerLikelihood += Math.log(fHupAvp);
						// System.out.println("G-fHupAvp:"+fHupAvp);

					}
				}
			}
		}

		// Second term in eqn 24. Compute non follower likelihood.
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int u = currUser.nonFollowers[i].followerIndex;
					User nonFollower = dataset.users[u];
					int nonFollowerPlatform = currUser.nonFollowers[i].platform;

					// only consider this user if he exist in the platform and
					// nonfollower relationships in the platform
					if (currUser.platforms[p] == 1 && nonFollowerPlatform == p) {
						// Compute H_u * A_v
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							HupAvp += nonFollower.hubs[z] * nonFollower.topicalPlatformPreference[z][p] * x[z]
									* currUser.topicalPlatformPreference[z][p];// now
							// A_v
							// is
							// x
						}
						HupAvp = HupAvp * lamda;
						double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
						nonFollowerLikelihood += Math.log(1 - fHupAvp);
					}
				}
			}
		}
		// Third term in eqn 24. Compute post likelihood.
		for (int k = 0; k < nTopics; k++) {
			postLikelihood += ((sigma - 1) * Math.log(x[k])) - ((x[k] * sigma) / currUser.topicalInterests[k]);// now
			// A_v
			// is
			// x
		}

		likelihood = nonFollowerLikelihood + followerLikelihood + postLikelihood;

		return likelihood;
	}

	/***
	 * compute gradient of likelihood of data with respect to authority of u in
	 * topic k when the authority is x, i.e., if if L(data|parameters) = f(A_u)
	 * + const-of-A_u then this function return df/dA_uk at A_uk = x
	 * 
	 * @param v
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_authority(int v, int k, double x) {
		// Refer to Eqn 26 in Learning paper
		double followerLikelihood = 0;
		double nonFollowerLikelihood = 0;
		double postLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[v];

		// First term in eqn 26. Compute follower likelihood
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int u = currUser.followers[i].followerIndex;
					User follower = dataset.users[u];
					int followerPlatform = currUser.followers[i].platform;
					// only consider this user if he exist in the platform and
					// the follower relationship is in this platform
					if (currUser.platforms[p] == 1 && followerPlatform == p) {

						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += follower.hubs[z] * follower.topicalPlatformPreference[z][p] * x
										* currUser.topicalPlatformPreference[z][p];
							} else {
								HupAvp += follower.hubs[z] * follower.topicalPlatformPreference[z][p]
										* currUser.authorities[z] * currUser.topicalPlatformPreference[z][p];
							}
						}
						HupAvp = HupAvp * lamda;

						followerLikelihood += (1 / (1 - Math.exp(-HupAvp)) * -Math.exp(-HupAvp) * -lamda
								* follower.topicalPlatformPreference[k][p] * follower.hubs[k]
										* currUser.topicalPlatformPreference[k][p])
								- (1 / (Math.exp(-HupAvp) + 1) * Math.exp(-HupAvp) * -lamda
										* follower.topicalPlatformPreference[k][p] * follower.hubs[k]
												* currUser.topicalPlatformPreference[k][p]);
					}
				}
			}
		}

		// Second term in eqn 26. Compute non follower likelihood
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int u = currUser.nonFollowers[i].followerIndex;
					User nonFollower = dataset.users[u];
					int nonFollowerPlatform = currUser.nonFollowers[i].platform;

					// only consider this user if he exist in the platform and
					// the follower relationship is in this platform
					if (currUser.platforms[p] == 1 && nonFollowerPlatform == p) {
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += nonFollower.hubs[z] * nonFollower.topicalPlatformPreference[z][p] * x
										* currUser.topicalPlatformPreference[z][p];
							} else {
								HupAvp += nonFollower.hubs[z] * nonFollower.topicalPlatformPreference[z][p]
										* currUser.authorities[z] * currUser.topicalPlatformPreference[z][p];
							}
						}
						HupAvp = HupAvp * lamda;

						nonFollowerLikelihood += (-lamda * currUser.topicalPlatformPreference[k][p]
								* nonFollower.hubs[k] * nonFollower.topicalPlatformPreference[k][p])
								- ((1 / (Math.exp(-HupAvp) + 1)) * (Math.exp(-HupAvp))
										* (-lamda * nonFollower.hubs[k] * nonFollower.topicalPlatformPreference[k][p]
												* currUser.topicalPlatformPreference[k][p]));
					}
				}
			}
		}

		// Third term in eqn 26. Compute post likelihood
		postLikelihood = ((sigma - 1) / x) - (sigma / currUser.topicalInterests[k]);

		gradLikelihood = nonFollowerLikelihood + followerLikelihood + postLikelihood;

		return gradLikelihood;
	}

	/***
	 * alternating step to optimize authorities of user u
	 * 
	 * @param u
	 */
	private void altOptimize_Authorities(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].authorities;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_authority(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		for (int iter = 0; iter < maxIteration_Authorities; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_authority(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
					if (x[k] < epsilon) {
						x[k] = epsilon;
					}
				}

				// compute f at the new x
				f = 0 - getLikelihood_authority(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				 System.out.printf("alt_authority: u = %d iter = %d f = %f\n",
				 u, iter, f);
			} else {
				// to see if F actually reduce after every iteration
				 System.out.printf("alt_authority: u = %d iter = %d f = %f\n",
				 u, iter, f);
				break;// cannot improve further
			}
		}

	}

	/***
	 * compute likelihood of data as a function of hub of u when the hub is x,
	 * i.e., if L(data|parameters) = f(H_u) + const-of-H_u then this function
	 * returns f(x)
	 * 
	 * @param u
	 * @param x[]
	 * @return
	 */
	private double getLikelihood_hub(int u, double[] x) {
		// Refer to Eqn 20 in learning paper
		double followingLikelihood = 0;
		double nonFollowingLikelihood = 0;
		double postLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[u];

		// First term in eqn 20. Compute following likelihood.
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.followings[i].followingIndex;
					User following = dataset.users[v];
					int followingPlatform = currUser.followings[i].platform;

					// only consider this user if he exist in the platform
					if (currUser.platforms[p] == 1 && followingPlatform == p) {
						// Compute H_u^p * A_v^p
							double HupAvp = 0;
							for (int z = 0; z < nTopics; z++) {
								HupAvp += x[z] * currUser.topicalPlatformPreference[z][p] * following.authorities[z]
										* following.topicalPlatformPreference[z][p];// now
								// H_u
								// is
								// x
							}
							HupAvp = HupAvp * lamda;
							double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
							followingLikelihood += Math.log(fHupAvp);
							//followingLikelihood += Math.log(1-Math.exp(HupAvp)) - Math.log(Math.exp(HupAvp)+1);
							//System.out.println(Math.log(1-Math.exp(HupAvp)) - Math.log(Math.exp(HupAvp)+1));
						
					}
				}
			}
		}
		// Second term in eqn 20. Compute non following likelihood.
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.nonFollowings[i].followingIndex;
					User nonFollowing = dataset.users[v];
					int nonFollowingPlatform = currUser.nonFollowings[i].platform;
					// only consider this user if he exist in the platform and nonfollowing relationships in the platform
					if (currUser.platforms[p] == 1 && nonFollowingPlatform == p) {
						// Compute H_u * A_v
							double HupAvp = 0;
							for (int z = 0; z < nTopics; z++) {
								HupAvp += x[z] * currUser.topicalPlatformPreference[z][p]
										* nonFollowing.authorities[z] * nonFollowing.topicalPlatformPreference[z][p];// now
								// H_u
								// is
								// x
							}
							HupAvp = HupAvp * lamda;
							double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
							nonFollowingLikelihood += Math.log(1 - fHupAvp);
							//nonFollowingLikelihood +=Math.log(2) + HupAvp - Math.log(Math.exp(HupAvp) + 1);
							//System.out.println(Math.log(2) + HupAvp - Math.log(Math.exp(HupAvp) + 1));
						
					}
				}
			}
		}
		
		
		// Third term in eqn 20. Compute post likelihood.
		for (int k = 0; k < nTopics; k++) {
			postLikelihood += ((delta - 1) * Math.log(x[k])) - ((x[k] * delta) / currUser.topicalInterests[k]);// now
			// H_u
			// is
			// x
		}

		likelihood = nonFollowingLikelihood + followingLikelihood + postLikelihood;
		//likelihood = followingLikelihood;
		
		return likelihood;
	}
	
	/***
	 * compute gradient of likelihood of data with respect to hub of u in topic
	 * k when the hub is x, i.e., if if L(data|parameters) = f(H_u) +
	 * const-of-H_u then this function return df/dH_uk at H_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_hub(int u, int k, double x) {
		// Refer to Eqn 22 in learning paper
		double followingLikelihood = 0;
		double nonFollowingLikelihood = 0;
		double postLikelihood = 0;
		double gradLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		// First term in eqn 22. Compute following likelihood
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.followings[i].followingIndex;
					User following = dataset.users[v];
					int followingPlatform = currUser.followings[i].platform;

					// only consider this user if he exist in the platform and
					// the follower relationship is in this platform
					if (currUser.platforms[p] == 1 && followingPlatform == p) {

						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += x * currUser.topicalPlatformPreference[z][p] * following.authorities[z]
										* following.topicalPlatformPreference[z][p];
							} else {
								HupAvp += currUser.hubs[z] * currUser.topicalPlatformPreference[z][p]
										* following.authorities[z] * following.topicalPlatformPreference[z][p];
							}
						}
						HupAvp = HupAvp * lamda;

						followingLikelihood += (1 / (1 - Math.exp(-HupAvp))
								* -Math.exp(-HupAvp)
								* (-lamda * following.authorities[k] * following.topicalPlatformPreference[k][p]
										* currUser.topicalPlatformPreference[k][p]))
								- (1 / (Math.exp(-HupAvp) + 1)
										* Math.exp(-HupAvp)
										* (-lamda * following.authorities[k] * following.topicalPlatformPreference[k][p]
												* currUser.topicalPlatformPreference[k][p]));
					}
				}
			}
		}
		
		// Second term in eqn 22. Compute non following likelihood
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.nonFollowings[i].followingIndex;
					User nonFollowing = dataset.users[v];
					int nonFollowingPlatform = currUser.nonFollowings[i].platform;

					// only consider this user if he exist in the platform and
					// the follower relationship is in this platform
					if (currUser.platforms[p] == 1 && nonFollowingPlatform == p) {
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += x * currUser.topicalPlatformPreference[z][p] * nonFollowing.authorities[z]
										* nonFollowing.topicalPlatformPreference[z][p];
							} else {
								HupAvp += currUser.hubs[z] * currUser.topicalPlatformPreference[z][p]
										* nonFollowing.authorities[z] * nonFollowing.topicalPlatformPreference[z][p];
							}
						}
						HupAvp = HupAvp * lamda;

						nonFollowingLikelihood += (-lamda * currUser.topicalPlatformPreference[k][p] * nonFollowing.authorities[k] * nonFollowing.topicalPlatformPreference[k][p])
								- ((1 / (Math.exp(-HupAvp) + 1))
										* (Math.exp(-HupAvp))
										* (-lamda * nonFollowing.authorities[k]* nonFollowing.topicalPlatformPreference[k][p]* currUser.topicalPlatformPreference[k][p]));
					}
				}
			}
		}
		// Third term in eqn 22. Compute post likelihood
		postLikelihood = ((delta - 1) / x) - (delta / currUser.topicalInterests[k]);

		gradLikelihood = nonFollowingLikelihood + followingLikelihood + postLikelihood;
		//gradLikelihood = followingLikelihood;
		
		
		return gradLikelihood;
	}
	
	/***
	 * alternating step to optimize hubs of user u
	 * 
	 * @param u
	 */
	private void altOptimize_Hubs(int u) {
		double[] grad = new double[nTopics];
		double[] currentX = dataset.users[u].hubs;
		double[] x = new double[nTopics];

		double currentF = 0 - getLikelihood_hub(u, currentX);

		boolean flag = true;
		double diff = 0;
		double f = Double.MAX_VALUE;

		for (int iter = 0; iter < maxIteration_Hubs; iter++) {
			// compute gradient
			for (int k = 0; k < nTopics; k++) {
				grad[k] = 0 - gradLikelihood_hub(u, k, currentX[k]);
			}
			// start line search
			lineSearch_lambda = lineSearch_beta;
			flag = false;

			for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
				// find new x
				for (int k = 0; k < nTopics; k++) {
					x[k] = currentX[k] - lineSearch_lambda * grad[k];
					if (x[k] < epsilon) {
						x[k] = epsilon;
					}
				}

				// compute f at the new x
				f = 0 - getLikelihood_hub(u, x);

				// compute ||currentX - x||^2
				diff = 0;
				for (int k = 0; k < nTopics; k++) {
					diff += Math.pow(currentX[k] - x[k], 2);
				}
				// check the condition to stop line search
				if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
					flag = true;
					break;
				} else {
					lineSearch_lambda *= lineSearch_beta;
				}
			}
			if (flag) {// line search successful
				currentF = f;
				for (int k = 0; k < nTopics; k++) {
					currentX[k] = x[k];
				}
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u,
				// iter, f);
			} else {
				// to see if F actually reduce after every iteration
				// System.out.printf("alt_hub: u = %d iter = %d f = %f\n", u,
				// iter, f);
				break;// cannot improve further
			}
		}
	}
	
	/***
	 * compute likelihood of data as a function of platform preference for topic
	 * k of u when the preference is x, i.e., if L(data|parameters) = f(Eta_uk)
	 * + const-of-Eta_uk then this function returns f(x)
	 * 
	 * @param u
	 * @return
	 */
	private double getLikelihood_platformPreference(int u, int k, double[] x) {
		// Refer to Eqn 28 in Learning paper for Formula
		double linkLikelihood = 0;
		double nonLinkLikelihood = 0;
		double postLikelihood = 0;
		double platformLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[u];

		// First term in eqn 28. Compute link likelihood.
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.followings[i].followingIndex;
					User following = dataset.users[v];
					int followingPlatform = currUser.followings[i].platform;

					// only consider this user if he exist in the platform
					if (currUser.platforms[p] == 1) {
						// only consider follower relationships in the platform
						if (followingPlatform == p) {
							// Compute H_u^p * A_v^p
							double HupAvp = 0;
							for (int z = 0; z < nTopics; z++) {
								if (z == k) {
									HupAvp += currUser.hubs[z] * x[p] * following.authorities[z]
											* following.topicalPlatformPreference[z][p];// now
									// Eta_u,k
									// is
									// x
								} else {
									HupAvp += currUser.hubs[z] * currUser.topicalPlatformPreference[z][p]
											* following.authorities[z] * following.topicalPlatformPreference[z][p];
								}

							}
							HupAvp = HupAvp * lamda;
							double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
							linkLikelihood += Math.log(fHupAvp);
						}
					}
				}
			}
		}
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.followers[i].followerIndex;
					User follower = dataset.users[v];
					int followerPlatform = currUser.followers[i].platform;

					// only consider this user if he exist in the platform
					if (currUser.platforms[p] == 1) {
						// only consider follower relationships in the platform
						if (followerPlatform == p) {
							// Compute H_u^p * A_v^p
							double HupAvp = 0;
							for (int z = 0; z < nTopics; z++) {
								if (z == k) {
									HupAvp += follower.hubs[z] * follower.topicalPlatformPreference[z][p]
											* currUser.authorities[z] * x[p];// now
									// Eta_u,k
									// is
									// x
								} else {
									HupAvp += follower.hubs[z] * follower.topicalPlatformPreference[z][p]
											* currUser.authorities[z] * currUser.topicalPlatformPreference[z][p];
								}
							}
							HupAvp = HupAvp * lamda;
							double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
							linkLikelihood += Math.log(fHupAvp);
						}
					}
				}
			}
		}

		// Second term in eqn 28. Compute non link likelihood.
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.nonFollowings[i].followingIndex;
					User nonFollowing = dataset.users[v];
					int nonFollowingPlatform = currUser.nonFollowings[i].platform;

					// only consider this user if he exist in the platform
					if (currUser.platforms[p] == 1) {
						// only consider follower relationships in the platform
						if (nonFollowingPlatform == p) {
							// Compute H_u^p * A_v^p
							double HupAvp = 0;
							for (int z = 0; z < nTopics; z++) {
								if (z == k) {
									HupAvp += currUser.hubs[z] * x[p] * nonFollowing.authorities[z]
											* nonFollowing.topicalPlatformPreference[z][p];// now
									// Eta_u,k
									// is
									// x
								} else {
									HupAvp += currUser.hubs[z] * currUser.topicalPlatformPreference[z][p]
											* nonFollowing.authorities[z]
													* nonFollowing.topicalPlatformPreference[z][p];
								}
							}
							HupAvp = HupAvp * lamda;
							double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
							nonLinkLikelihood += Math.log(1 - fHupAvp);
						}
					}
				}
			}
		}
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					int v = currUser.nonFollowers[i].followerIndex;
					User nonFollower = dataset.users[v];
					int nonFollowerPlatform = currUser.nonFollowers[i].platform;

					// only consider this user if he exist in the platform
					if (currUser.platforms[p] == 1) {
						// only consider follower relationships in the platform
						if (nonFollowerPlatform == p) {
							// Compute H_u^p * A_v^p
							double HupAvp = 0;
							for (int z = 0; z < nTopics; z++) {
								if (z == k) {
									HupAvp += nonFollower.hubs[z] * nonFollower.topicalPlatformPreference[z][p]
											* currUser.authorities[z] * x[p];// now
									// Eta_u,k
									// is
									// x
								} else {
									HupAvp += nonFollower.hubs[z] * nonFollower.topicalPlatformPreference[z][p]
											* currUser.authorities[z] * currUser.topicalPlatformPreference[z][p];
								}
							}
							HupAvp = HupAvp * lamda;
							double fHupAvp = 2 * ((1 / (Math.exp(-HupAvp) + 1)) - 0.5);
							nonLinkLikelihood += Math.log(1 - fHupAvp);
							//System.out.println(fHupAvp);
						}
					}
				}
			}
		}

		// Third term in eqn 28. Compute post likelihood.
		for (int s = 0; s < currUser.nPosts; s++) {
			int z = currUser.posts[s].topic;
			int currP = currUser.posts[s].platform;
			if (currUser.postBatches[s] == batch) {
				double denominator = 0;
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					if (z==k){
						denominator += Math.exp(x[p]);
					} else {
						denominator += Math.exp(currUser.topicalPlatformPreference[z][p]);
					}
				}
				if (z==k){
					postLikelihood += Math.log(Math.exp(x[currP]) / denominator);
				} else {
					postLikelihood += Math.log(Math.exp(currUser.topicalPlatformPreference[z][currP]) / denominator);
				}
			}
		}

		// Fourth term in eqn 28. Compute platform likelihood.
		for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
			platformLikelihood += ((alpha - 1) * Math.log(x[p])) - (x[p] / theta);
		}

		//likelihood = linkLikelihood + nonLinkLikelihood + postLikelihood + platformLikelihood;
		likelihood = nonLinkLikelihood;

		return likelihood;
	}
	
	/***
	 * compute gradient of likelihood of data with respect to platform
	 * preference of u in topic k when the preference is x, i.e., if if
	 * L(data|parameters) = f(Eta_uk) + const-of-Eta_uk then this function
	 * return df/dEta_ukp at Eta_ukp = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_platformPreference(int u, int k, int p, double x) {
		// Refer to Eqn 31 in Learning paper
		double linkLikelihood = 0;
		double nonLinkLikelihood = 0;
		double postLikelihood = 0;
		double platformLikelihood = 0;
		double likelihood = 0;

		// Set the current user to be v
		User currUser = dataset.users[u];

		// First term in eqn 31. Compute link likelihood.
		if (currUser.followings != null) {
			for (int i = 0; i < currUser.followings.length; i++) {
				int v = currUser.followings[i].followingIndex;
				User following = dataset.users[v];
				int followingPlatform = currUser.followings[i].platform;
				// only consider this user if he exist in the platform
				if (currUser.platforms[p] == 1) {
					// only consider follower relationships in the platform
					if (followingPlatform == p) {
						// Compute H_u^p * A_v^p
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += currUser.hubs[z] * x * following.authorities[z]
										* following.topicalPlatformPreference[z][p];// now
								// Eta_u,k,p
								// is
								// x
							} else {
								HupAvp += currUser.hubs[z] * currUser.topicalPlatformPreference[z][p]
										* following.authorities[z] * following.topicalPlatformPreference[z][p];
							}

						}
						HupAvp = HupAvp * lamda;
						
						linkLikelihood += (1 / (1 - Math.exp(-HupAvp))
								* -Math.exp(-HupAvp)
								* (-lamda * currUser.hubs[k] * following.authorities[k]
										* following.topicalPlatformPreference[k][p]))
								- (1 / (Math.exp(-HupAvp) + 1)
										* Math.exp(-HupAvp)
										* (-lamda * currUser.hubs[k] * following.authorities[k]
												* following.topicalPlatformPreference[k][p]));
					}
				}
			}
		}
		if (currUser.followers != null) {
			for (int i = 0; i < currUser.followers.length; i++) {
				int v = currUser.followers[i].followerIndex;
				User follower = dataset.users[v];
				int followerPlatform = currUser.followers[i].platform;
				// only consider this user if he exist in the platform
				if (currUser.platforms[p] == 1) {
					// only consider follower relationships in the platform
					if (followerPlatform == p) {
						// Compute H_u^p * A_v^p
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += follower.hubs[z] * follower.topicalPlatformPreference[z][p]
										* currUser.authorities[z] * x;// now
								// Eta_u,k
								// is x
							} else {
								HupAvp += follower.hubs[z] * follower.topicalPlatformPreference[z][p]
										* currUser.authorities[z] * currUser.topicalPlatformPreference[z][p];
							}
						}
						HupAvp = HupAvp * lamda;

						linkLikelihood += (1 / (1 - Math.exp(-HupAvp)) 
								* -Math.exp(-HupAvp) 
								* (-lamda * follower.hubs[k] * follower.topicalPlatformPreference[k][p]
										* currUser.authorities[k]))
								- (1 / (Math.exp(-HupAvp) + 1) *
										Math.exp(-HupAvp) 
										* (-lamda * follower.hubs[k] * follower.topicalPlatformPreference[k][p]
												* currUser.authorities[k]));
					}
				}
			}
		}

		// Second term in eqn 31. Compute non link likelihood.
		if (currUser.nonFollowings != null) {
			for (int i = 0; i < currUser.nonFollowings.length; i++) {
				int v = currUser.nonFollowings[i].followingIndex;
				User nonFollowing = dataset.users[v];
				int nonFollowingPlatform = currUser.nonFollowings[i].platform;
				// only consider this user if he exist in the platform
				if (currUser.platforms[p] == 1) {
					// only consider follower relationships in the platform
					if (nonFollowingPlatform == p) {
						// Compute H_u^p * A_v^p
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += currUser.hubs[z] * x * nonFollowing.authorities[z]
										* nonFollowing.topicalPlatformPreference[z][p];// now
								// Eta_u,k
								// is
								// x
							} else {
								HupAvp += currUser.hubs[z] * currUser.topicalPlatformPreference[z][p]
										* nonFollowing.authorities[z] * nonFollowing.topicalPlatformPreference[z][p];
							}
						}
						HupAvp = HupAvp * lamda;

						nonLinkLikelihood += (-lamda * currUser.hubs[k] * nonFollowing.authorities[k]
								* nonFollowing.topicalPlatformPreference[k][p])
								- ((1 / (Math.exp(-HupAvp) + 1))
										* (Math.exp(-HupAvp))
										* (-lamda * currUser.hubs[k] * nonFollowing.authorities[k]
												* nonFollowing.topicalPlatformPreference[k][p]));
					}
				}
			}
		}
		if (currUser.nonFollowers != null) {
			for (int i = 0; i < currUser.nonFollowers.length; i++) {
				int v = currUser.nonFollowers[i].followerIndex;
				User nonFollower = dataset.users[v];
				int nonFollowerPlatform = currUser.nonFollowers[i].platform;

				// only consider this user if he exist in the platform
				if (currUser.platforms[p] == 1) {
					// only consider follower relationships in the platform
					if (nonFollowerPlatform == p) {
						// Compute H_u^p * A_v^p
						double HupAvp = 0;
						for (int z = 0; z < nTopics; z++) {
							if (z == k) {
								HupAvp += nonFollower.hubs[z] * nonFollower.topicalPlatformPreference[z][p]
										* currUser.authorities[z] * x;// now
								// Eta_u,k
								// is x
							} else {
								HupAvp += nonFollower.hubs[z] * nonFollower.topicalPlatformPreference[z][p]
										* currUser.authorities[z] * currUser.topicalPlatformPreference[z][p];
							}
						}
						HupAvp = HupAvp * lamda;
						
						nonLinkLikelihood += (-lamda * nonFollower.hubs[k] * nonFollower.topicalPlatformPreference[k][p]
								* currUser.authorities[k])
								- ((1 / (Math.exp(-HupAvp) + 1)) * (Math.exp(-HupAvp))
										* (-lamda * nonFollower.hubs[k] * nonFollower.topicalPlatformPreference[k][p]
												* currUser.authorities[k]));
					}
				}
			}
		}

		// Third term in eqn 31. Compute post likelihood.
		double firstSubTerm = 0;
		double secondSubTerm = 0;
		for (int s = 0; s < currUser.nPosts; s++) {
			int z = currUser.posts[s].topic;
			int j = currUser.posts[s].platform;
			if (currUser.postBatches[s] == batch) {
				if (j == p) {
					firstSubTerm++;
				}
				double denominator = 0;
				for (int t = 0; t < Configure.NUM_OF_PLATFORM; t++) {
					if (z==k && t==p){
						denominator += Math.exp(x);
					} else {
						denominator += Math.exp(currUser.topicalPlatformPreference[z][t]);
					}	
				}
				if (z==k && j==p){
					secondSubTerm = (1 / denominator) * Math.exp(x);
				} else {
					
				}secondSubTerm = (1 / denominator) * Math.exp(currUser.topicalPlatformPreference[z][j]);
			}

		}
		postLikelihood += firstSubTerm - secondSubTerm;

		// Fourth term in eqn 28. Compute platform likelihood.
		platformLikelihood += ((alpha - 1) / x) - (1 / theta);

		//likelihood = linkLikelihood + nonLinkLikelihood + postLikelihood + platformLikelihood;
		likelihood =  nonLinkLikelihood;
		

		return likelihood;

	}
	
	/***
	 * alternating step to optimize platform preference of user u
	 * 
	 * @param u
	 */
	private void altOptimize_PlatformPreference(int u, int k) {
		// the topical platform preferences is a 2D array
		
			double[] grad = new double[Configure.NUM_OF_PLATFORM];
			double[] currentX = new double[Configure.NUM_OF_PLATFORM];
			double[] x = new double[Configure.NUM_OF_PLATFORM];

			// get the currentX
			for (int i = 0; i < dataset.users[u].topicalPlatformPreference[k].length; i++) {
				currentX[i] = dataset.users[u].topicalPlatformPreference[k][i];
			}

			double currentF = 0 - getLikelihood_platformPreference(u, k, currentX);

			boolean flag = true;
			double diff = 0;
			double f = Double.MAX_VALUE;

			for (int iter = 0; iter < maxIteration_platformPreference; iter++) {
				// compute gradient
				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					grad[p] = 0 - gradLikelihood_platformPreference(u, k, p, currentX[p]);
				}

				// start line search
				lineSearch_lambda = lineSearch_beta;
				flag = false;
				for (int lineSearchIter = 0; lineSearchIter < lineSearch_MaxIterations; lineSearchIter++) {
					// find new x
					for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
						x[p] = currentX[p] - lineSearch_lambda * grad[p];
					}
					// compute f at the new x
					f = 0 - getLikelihood_platformPreference(u, k, x);

					// compute ||currentX - x||^2
					diff = 0;
					for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
						diff += Math.pow(currentX[p] - x[p], 2);
					}
					// check the condition to stop line search
					if (f - currentF <= (-lineSearch_alpha / lineSearch_lambda) * diff) {
						flag = true;
						break;
					} else {
						lineSearch_lambda *= lineSearch_beta;
					}
				}
				if (flag) {// line search successful
					currentF = f;
					for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
						currentX[p] = x[p];
					}
					// to see if F actually reduce after every iteration
					// System.out.printf("alt_hub: u = %d iter = %d f = %f\n",
					// u,
					// iter, f);
				} else {
					// to see if F actually reduce after every iteration
					// System.out.printf("alt_hub: u = %d iter = %d f = %f\n",
					// u,
					// iter, f);
					break;// cannot improve further
				}
			}
	}
	
	/***
	 * to sample topic for post n of user u
	 * 
	 * @param u
	 * @param n
	 */
	private void samplePostTopic_EMGibbs(int u, int n) {
		// Refer to Eqn 32 in Learning paper

		// Set the current user to be u
		User currUser = dataset.users[u];

		double sump = 0;
		// p: p(z_u,s = z| rest)

		double[] p = new double[nTopics];
		double max = -Double.MAX_VALUE;

		// Softmax to covert the user topical interests to between 0-1
		double[] currUserTopicalInterests = tool.MathTool.softmax(currUser.topicalInterests);

		for (int z = 0; z < nTopics; z++) {
			// User-topic
			p[z] = Math.log(currUserTopicalInterests[z]);

			// topic-word
			Post currPost = currUser.posts[n];
			for (int w = 0; w < currPost.words.length; w++) {
				int word = currPost.words[w];
				p[z] += Math.log(topicWordDist[z][word]);
			}

			// update min
			if (max < p[z]) {
				max = p[z];
			}

		}
		double[] plog = new double[nTopics];
		for (int z = 0; z < nTopics; z++) {
			plog[z] = p[z];
		}
		// convert log(sump) to probability
		for (int z = 0; z < nTopics; z++) {
			p[z] = p[z] - max;
			p[z] = Math.exp(p[z]);

			// cumulative
			p[z] = sump + p[z];
			sump = p[z];
		}

		sump = rand.nextDouble() * sump;
		for (int z = 0; z < nTopics; z++) {
			if (sump > p[z]) {
				continue;
			} else {
				// Sample topic
				currUser.posts[n].topic = z;
				/*
				 * if (currUser.topicalInterests[z] <= 10E-12) {
				 * System.err.println("Something wrong!!! "); for (int k = 0; k
				 * < nTopics; k++) { System.out.printf(
				 * "theta[%d] = %.12f \tlog(theta[%d]) = %.12f \tplog[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n"
				 * , k, currUser.topicalInterests[k], k,
				 * Math.log(currUser.topicalInterests[k]), k, plog[k], k, p[k],
				 * sump); } System.out.printf("z = %d\n", z); System.exit(-1); }
				 */
				return;
			}
		}
		System.err.println("Something wrong!!! ");
		for (int k = 0; k < nTopics; k++) {
			System.out.printf("theta[%d] = %.12f \t p[%d] = %.12f sump = %.12f\n", k, currUser.topicalInterests[k], k,
					p[k], sump);
		}
		System.exit(-1);
	}
	
	/***
	 * alternating step to optimize topics' word distribution
	 */
	private void altOptimize_topics() {
		// initialize the count variables
		for (int k = 0; k < nTopics; k++) {
			sum_nzw[k] = 0;
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				n_zw[k][w] = 0;
			}
		}

		// update count variable base on the post topic assigned
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int n = 0; n < currUser.posts.length; n++) {
				Post currPost = currUser.posts[n];
				// only consider posts in batch
				if (currUser.postBatches[n] == batch) {
					int z = currPost.topic;
					for (int i = 0; i < currPost.nWords; i++) {
						int wordIndex = currPost.words[i];
						sum_nzw[z] += 1;
						n_zw[z][wordIndex] += 1;
					}
				}
			}
		}

		// compute topic word distribution
		for (int k = 0; k < nTopics; k++) {
			for (int w = 0; w < dataset.vocabulary.length; w++) {
				topicWordDist[k][w] = (n_zw[k][w] + gamma) / (sum_nzw[k] + gamma * dataset.vocabulary.length);
			}
		}
	}
	
	/***
	 * checking if the gradient computation of likelihood of user topical
	 * interest X_{u,k} is properly implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_TopicalInterest(int u, int k) {
		double DELTA = 1;

		double[] x = dataset.users[u].topicalInterests;

		double f = getLikelihood_topicalInterest(u, x);
		double g = gradLikelihood_topicalInterest(u, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_topicalInterest(u, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(String.format("[TopicInterest] u = %d k = %d DELTA = %.12f numGrad = %f grad = %f\n", u,
					k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;
		}
	}
	
	/***
	 * checking if the gradient computation of likelihood of A_{v,k} is properly
	 * implemented
	 * 
	 * @param v
	 * @param k
	 */
	public void gradCheck_Authority(int v, int k) {
		double DELTA = 1;

		double[] x = dataset.users[v].authorities;
		double f = getLikelihood_authority(v, x);
		double g = gradLikelihood_authority(v, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_authority(v, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(String.format("[Authority] v= %d k = %d DELTA = %f numGrad = %f grad = %f\n", v, k, DELTA,
					numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;
		}
	}
	
	/***
	 * checking if the gradient computation of likelihood of H_{u,k} is properly
	 * implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_Hub(int u, int k) {
		double DELTA = 1;

		double[] x = dataset.users[u].hubs;

		double f = getLikelihood_hub(u, x);
		double g = gradLikelihood_hub(u, k, x[k]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[k] += DELTA;
			double DELTAF = getLikelihood_hub(u, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(
					String.format("[Hub] u = %d k = %d DELTA = %f numGrad = %f grad = %f\n", u, k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[k] -= DELTA;

		}
	}
	
	/***
	 * checking if the gradient computation of likelihood of H_{u,k} is properly
	 * implemented
	 * 
	 * @param u
	 * @param k
	 */
	public void gradCheck_PlatformPreference(int u, int k, int p) {
		double DELTA = 1;

		double[] x = new double[Configure.NUM_OF_PLATFORM];
		for (int i = 0; i < Configure.NUM_OF_PLATFORM; i++) {
			x[i] = dataset.users[u].topicalPlatformPreference[k][i];
		}

		double f = getLikelihood_platformPreference(u, k, x);
		double g = gradLikelihood_platformPreference(u, k, p, x[p]);

		for (int i = 1; i <= 20; i++) {
			// reduce DELTA
			DELTA *= 0.1;
			x[p] += DELTA;
			double DELTAF = getLikelihood_platformPreference(u, k, x);
			double numGrad = (DELTAF - f) / DELTA;
			System.out.printf(
					String.format("[Hub] u = %d k = %d DELTA = %f numGrad = %f grad = %f\n", u, k, DELTA, numGrad, g));
			// if grad function is implemented properly, we will see numGrad
			// gets closer to grad
			x[p] -= DELTA;

		}
	}

	/***
	 * initialize the data before training
	 */
	private void init() {
		// alpha = (double) (20) / (double) (nTopics);// prior for users'
		// interest
		gamma = 0.001;// prior for word distribution
		sigma = 2.0;// shape parameter of users' authorities
		delta = 2.0;// shapa parameter of users' hubs
		kappa = 2.0;// shape parameter of user interest latent vector
		alpha = 2.0;// shape parameter of user platform preference vector
		theta = 2.0;// scale parameter of user interests/platform preference
		// vectors
		rand = new Random();

		// initialize the count variables
		for (int u = 0; u < dataset.nUsers; u++) {
			sum_nzu[u] = 0;
			for (int k = 0; k < nTopics; k++) {
				n_zu[k][u] = 0;
			}
		}

		// randomly assign topics to posts
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			for (int n = 0; n < currUser.posts.length; n++) {
				// only consider posts in batch
				if (currUser.postBatches[n] == batch) {
					int randTopic = rand.nextInt(nTopics);
					currUser.posts[n].topic = randTopic;
					sum_nzu[u] += 1;
					n_zu[randTopic][u] += 1;
				}
			}
		}

		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			currUser.topicalInterests = new double[nTopics];
			currUser.topicalPlatformPreference = new double[nTopics][Configure.NUM_OF_PLATFORM];
			for (int k = 0; k < nTopics; k++) {

				GammaDistribution g = new GammaDistribution(kappa, theta);
				currUser.topicalInterests[k] = g.sample();

				for (int p = 0; p < Configure.NUM_OF_PLATFORM; p++) {
					if (currUser.platforms[p] == 1) {
						g = new GammaDistribution(alpha, theta);
						currUser.topicalPlatformPreference[k][p] = g.sample();
					} else {
						currUser.topicalPlatformPreference[k][p] = 0;
					}
				}
			}
		}

		// randomly regress user's topical interest to initialize authority and
		// hub
		for (int u = 0; u < dataset.nUsers; u++) {
			User currUser = dataset.users[u];
			currUser.authorities = new double[nTopics];
			currUser.hubs = new double[nTopics];
			for (int k = 0; k < nTopics; k++) {

				GammaDistribution g = new GammaDistribution(sigma, currUser.topicalInterests[k] / sigma);
				currUser.authorities[k] = g.sample();

				g = new GammaDistribution(delta, currUser.topicalInterests[k] / delta);
				currUser.hubs[k] = g.sample();
			}
		}

		// compute topic words distribution base on the random topic assignment
		altOptimize_topics();

	}
	
	public void altCheck_TopicalInterest(int u) {
		altOptimize_topicalInterest(u);
	}

	public void altCheck_Authority(int u) {
		altOptimize_Authorities(u);
	}

	public void altCheck_Hub(int u) {
		altOptimize_Hubs(u);
	}

	/***
	 * modeling learning
	 */
	public void train() {
		init();
		double maxLikelihood = 0;
		double currentLikelihood = 0;
		System.out.println("Datapath:" + this.datapath);
		System.out.println("Alpha:" + this.alpha);
		System.out.println("Line Search Alpha:" + this.lineSearch_alpha);
		System.out.println("Line Search Beta:" + this.lineSearch_beta);
		System.out.println("Line Search Max Iterations:" + this.lineSearch_MaxIterations);
		System.out.println("#Topics:" + this.nTopics);
		System.out.println("#platforms:" + Configure.NUM_OF_PLATFORM);
		for (int iter = 0; iter < max_GibbsEM_Iterations; iter++) {
			// EM part that employs alternating optimization
			for (int u = 0; u < dataset.nUsers; u++) {
				altOptimize_topicalInterest(u);
			}
			for (int u = 0; u < dataset.nUsers; u++) {
				for (int k = 0; k < nTopics; k++) {
					altOptimize_PlatformPreference(u,k);
				}
			}
			for (int u = 0; u < dataset.nUsers; u++) {
				altOptimize_Authorities(u);
			}
			for (int u = 0; u < dataset.nUsers; u++) {
				altOptimize_Hubs(u);
			}
			
			altOptimize_topics();
			// Gibbs part
			for (int u = 0; u < dataset.nUsers; u++) {
				for (int n = 0; n < dataset.users[u].nPosts; n++) {
					// only consider posts in batch
					if (dataset.users[u].postBatches[n] == batch) {
						samplePostTopic_EMGibbs(u, n);
					}
				}
			}
			// set first Likelihood as the maxLikelihood
			currentLikelihood = getLikelihood();
			if (iter == 0) {
				maxLikelihood = currentLikelihood;
			} else {
				if (maxLikelihood < currentLikelihood) {
					maxLikelihood = currentLikelihood;
					// set optimized topicWordDist to be the current
					// TopicWordsDist
					optTopicWordDist = topicWordDist;
					// set optimized user topical interest, authority and hub
					for (int u = 0; u < dataset.nUsers; u++) {
						User currUser = dataset.users[u];
						currUser.optTopicalInterests = currUser.topicalInterests;
						currUser.optAuthorities = currUser.authorities;
						currUser.optHubs = currUser.hubs;
						currUser.optTopicalPlatformPreference = currUser.topicalPlatformPreference;
					}
				}
			}
			System.out.printf("likelihood after %d steps: %f, max %f ", iter, currentLikelihood, maxLikelihood);
			System.out.println();
		}
		// print out the learned parameters
		output_topicWord();
		output_topicInterest();
		output_platformPreference();
		output_authority();
		output_hub();
		outputPostTopicTopWords(20);
	}
	
	public void output_topicWord() {
		try {
			File f = new File(dataset.path + "/" + nTopics + "/l_topicalWordDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int k = 0; k < nTopics; k++) {
				String text = Integer.toString(k);
				for (int w = 0; w < dataset.vocabulary.length; w++) {
					text = text + "," + Double.toString(optTopicWordDist[k][w]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical word file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	private void outputPostTopicTopWords(int k) {
		try {
			File f = new File(dataset.path + "/" + nTopics + "/l_topTopicWords.csv");
			BufferedWriter bw = new BufferedWriter(new FileWriter(f.getAbsoluteFile()));
			RankingTool rankTool = new RankingTool();
			WeightedElement[] topWords = null;
			for (int z = 0; z < nTopics; z++) {
				bw.write(z + "\n");
				topWords = rankTool.getTopKbyWeight(dataset.vocabulary, optTopicWordDist[z], k);
				for (int j = 0; j < k; j++)
					bw.write("," + topWords[j].name + "," + topWords[j].weight + "\n");
			}
			bw.close();
		} catch (Exception e) {
			System.out.println("Error in writing out post topic top words to file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	public void output_topicInterest() {
		try {
			File f = new File(dataset.path + "/" + nTopics + "/l_userTopicalInterestDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optTopicalInterests[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	public void output_platformPreference() {
		try {
			File f = new File(dataset.path + "/" + nTopics + "/l_userTopicalPlatformPreferenceDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					for (int p=0; p<Configure.NUM_OF_PLATFORM; p++){
						text = text + "," + p + "," + k + "," + Double.toString(currUser.optTopicalPlatformPreference[k][p]);
					}
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to platform preference file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	public void output_authority() {
		try {
			File f = new File(dataset.path + "/" + nTopics + "/l_userAuthorityDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optAuthorities[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to authority file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	public void output_hub() {
		try {
			File f = new File(dataset.path + "/" + nTopics + "/l_userHubDistributions.csv");
			FileWriter fo = new FileWriter(f);
			for (int u = 0; u < dataset.nUsers; u++) {
				User currUser = dataset.users[u];
				String text = currUser.userId;
				for (int k = 0; k < nTopics; k++) {
					text = text + "," + Double.toString(currUser.optHubs[k]);
				}
				fo.write(text + "\n");
			}
			fo.close();
		} catch (Exception e) {
			System.out.println("Error in writing to topical interest file!");
			e.printStackTrace();
			System.exit(0);
		}
	}
	
	public static void main(String[] args) {
		String datasetPath = "E:/users/roylee.2013/Chardonnay/synthetic/data/";
		// String datasetPath = "/Users/roylee/Documents/Chardonnay/mp-hat/syn_data/";
		int nTopics = 10;
		int batch = 1;

		MPHAT model = new MPHAT(datasetPath, nTopics, batch);
		model.init();

		Random rand = new Random();

		int u = rand.nextInt(100);
		int k = rand.nextInt(nTopics);

		// model.altCheck_TopicalInterest(u);
		 model.altCheck_Authority(u);
		// model.altCheck_Hub(u);
		// model.train();
		// model.gradCheck_Authority(u, k);
		// model.gradCheck_Hub(u, k);
		// model.gradCheck_TopicalInterest(u, k);
		// model.gradCheck_PlatformPreference(u,k,1);
	}

}
