package model;

import tool.*;

import java.io.BufferedWriter;
import java.util.Arrays;
import java.util.Random;
import org.apache.commons.math3.distribution.NormalDistribution;
import java.io.File;
import java.io.FileWriter;

public class MPHAT {
	public String datapath;
	public Dataset dataset;
	public int nTopics;
	public int batch;

	// priors

	public double alpha;// prior for users' platform preferences
	public double kappa;// prior for user's topic interests
	public double theta;
	public double sigma;// variance of users' authorities
	public double delta;// variance of users' hubs
	public double gamma; // variance of topic word distribution
	public double epsilon = 0.000001;
	public double lamda = 0.01;

	public Random rand;

	// Gibbs sampling variables
	// user-topic counts
	public int[][] n_zu = null; // n_zu[z][u]: number of times topic z is
								// observed in posts by user u
	public int[] sum_nzu = null; // sum_nzu[u] total number of topics that are
									// observed in posts by user u

	// topic-word counts
	public int[][] n_zw = null; // n_wz[z][w]: number of times word w is
								// generated by topic z in a post
	public int[] sum_nzw = null; // sum_nwz[z]: total number of times words that
									// are generated by topic z in a post

	// topic-word distribution
	public double[][] topicWordDist = null; // topicWordDist[k][w]: the
											// distribution of word w for topic
											// k. Sum of each words distribution
											// for each k = 1

	public double[][] optTopicWordDist = null; // optimized topicWordDist[k][w]

	//

	// options for learning
	public double lineSearch_alpha = 0.0001;
	public double lineSearch_beta = 0.1;
	public int lineSearch_MaxIterations = 10;;
	public double lineSearch_lambda;

	public int maxIteration_topicalInterest = 10;
	public int maxIteration_Authorities = 10;
	public int maxIteration_Hubs = 10;

	public int max_GibbsEM_Iterations = 500;

	/***
	 * 
	 * @param _datasetPath
	 * @param _nTopics
	 */
	public MPHAT(String _datasetPath, int _nTopics, int _batch) {
		this.datapath = _datasetPath;
		this.dataset = new Dataset(_datasetPath, _batch, false);
		this.nTopics = _nTopics;
		this.batch = _batch;
		n_zu = new int[nTopics][dataset.nUsers];
		sum_nzu = new int[dataset.nUsers];
		n_zw = new int[nTopics][dataset.vocabulary.length];
		sum_nzw = new int[nTopics];
		topicWordDist = new double[nTopics][dataset.vocabulary.length];
	}

	/***
	 * get likelihood of the whole dataset
	 * 
	 * @return
	 */
	private double getLikelihood() {
		// to be written
		// Compute the likelihood to make sure that it is improving L(text) +
		// L(link)
		// value can be more than 1
		// sum of eqn 1 -10
		return 0;

	}

	/***
	 * compute likelihood of data as a function of topical interest of u when
	 * the interest is x, i.e., if L(data|parameters) = f(X_u) + const-of-X_u
	 * then this function returns f(x)
	 * 
	 * @param u
	 * @return
	 */
	private double getLikelihood_topicalInterest(int u, double[] x) {
		// Refer to Eqn 16 in Learning paper for Formula
		
		double authorityLikelihood = 0;
		double hubLikelihood = 0;
		double postLikelihood = 0;
		double topicLikelihood = 0;
		double finalLikelihood = 0;

		// Set the current user to be u
		User currUser = dataset.users[u];

		for (int k = 0; k < nTopics; k++) {
			authorityLikelihood += -Math.pow((Math.log(currUser.authorities[k]) - x[k]), 2) / (2 * Math.pow(delta, 2));
		}

		for (int k = 0; k < nTopics; k++) {
			hubLikelihood += -Math.pow((Math.log(currUser.hubs[k]) - x[k]), 2) / (2 * Math.pow(sigma, 2));
		}

		for (int i = 0; i < currUser.nPosts; i++) {
			// Only compute post likelihood of posts which are in batch (i.e.
			// training batch = 1)
			if (currUser.postBatches[i] == batch) {
				int postTopic = currUser.posts[i].topic;
				// postLikelihood += x[postTopic];
				postLikelihood += Math.log(x[postTopic]);
			}
		}

		for (int k = 0; k < nTopics; k++) {
			topicLikelihood += (alpha - 1) * Math.log(x[k]);
		}
		
		finalLikelihood = authorityLikelihood + hubLikelihood + postLikelihood;
		return finalLikelihood;
		
	}

	/***
	 * compute gradient of likelihood of data with respect to interest of u in
	 * topic k when the interest is x, i.e., if if L(data|parameters) = f(X_u) +
	 * const-of-X_u then this function return df/dX_uk at X_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_topicalInterest(int u, int k, double x) {
		// Refer to Eqn 18 in Learning paper
		return 0;

	}

	/***
	 * compute likelihood of data as a function of platform preference for topic
	 * k of u when the preference is x, i.e., if L(data|parameters) = f(Eta_uk)
	 * + const-of-Eta_uk then this function returns f(x)
	 * 
	 * @param u
	 * @return
	 */
	private double getLikelihood_platformPreference(int u, double[] x) {
		// Refer to Eqn 28 in Learning paper for Formula
		return 0;
	}

	/***
	 * compute gradient of likelihood of data with respect to platform
	 * preference of u in topic k when the preference is x, i.e., if if
	 * L(data|parameters) = f(Eta_uk) + const-of-Eta_uk then this function
	 * return df/dEta_ukp at Eta_ukp = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_platformPreference(int u, int k, double x) {
		// Refer to Eqn 30 in Learning paper
		return 0;

	}

	private double[] simplexProjection(double[] x, double z) {
		// this will be replaced by the softmax function
		// Tuan-Anh: yes, this will be removed
		return null;
	}

	/***
	 * alternating step to optimize topical interest of u
	 * 
	 * @param u
	 */
	private void altOptimize_topicalInterest(int u) {

	}

	/***
	 * compute likelihood of data as a function of authority of u when the
	 * authority is x, i.e., if L(data|parameters) = f(A_u) + const-of-A_u then
	 * this function returns f(x)
	 * 
	 * @param v
	 * @param x[]
	 * @return
	 */
	private double getLikelihood_authority(int v, double[] x) {
		// Refer to Eqn 24 in Learning paper
		return 0;
	}

	/***
	 * compute gradient of likelihood of data with respect to authority of u in
	 * topic k when the authority is x, i.e., if if L(data|parameters) = f(A_u)
	 * + const-of-A_u then this function return df/dA_uk at A_uk = x
	 * 
	 * @param v
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_authority(int v, int k, double x) {
		// Refer to Eqn 26 in Learning paper
		return 0;
	}

	/***
	 * alternating step to optimize authorities of user u
	 * 
	 * @param u
	 */
	private void altOptimize_Authorities(int u) {

	}

	/***
	 * compute likelihood of data as a function of hub of u when the hub is x,
	 * i.e., if L(data|parameters) = f(H_u) + const-of-H_u then this function
	 * returns f(x)
	 * 
	 * @param u
	 * @param x[]
	 * @return
	 */
	private double getLikelihood_hub(int u, double[] x) {
		// Refer to Eqn 20 in learning paper
		return 0;
	}

	/***
	 * compute gradient of likelihood of data with respect to hub of u in topic
	 * k when the hub is x, i.e., if if L(data|parameters) = f(H_u) +
	 * const-of-H_u then this function return df/dH_uk at H_uk = x
	 * 
	 * @param u
	 * @param k
	 * @param x
	 * @return
	 */
	private double gradLikelihood_hub(int u, int k, double x) {
		// Refer to Eqn 22 in learning paper
		return 0;
	}

	/***
	 * alternating step to optimize hubs of user u
	 * 
	 * @param u
	 */
	private void altOptimize_Hubs(int u) {

	}

	/***
	 * alternating step to optimize topics' word distribution
	 */
	private void altOptimize_topics() {

	}

	/***
	 * to sample topic for post n of user u
	 * 
	 * @param u
	 * @param n
	 */
	private void sampleTopic(int u, int n) {
		// Are we still using gib sampling for this?
		// How about the platform selection for the post?
		// Tuan-Anh: yes, we use gibbs samling for this
		// Tuan-Anh: refer to Equation 31 in
	}

	/***
	 * alternating step to optimize platform preference of user u for topic k
	 * 
	 * @param u
	 * @param k
	 */
	private void altOptimize_PlatformPreference(int u, int k) {
		// Tuan-Anh: we need this function to learn users' topic-specific
		// platform preference
	}
}
